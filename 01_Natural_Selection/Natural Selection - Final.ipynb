{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Selection (Walmart) - SIOP 2019 Competition\n",
    "\n",
    "- Natural Selection = Natural Language Processing + Global Selection and Assessment\n",
    "\n",
    "This competition consisted of a data set containing open-ended resposes to 5 situational judgment items and 5 aggregated personality trait scores. The goal of the competition was to generate the best mean prediction across all 5 traits using only these open-ended responses.\n",
    "\n",
    "We used three approaches:\n",
    "- Key Words: a sample of responses from the high- and low-end of each trait distribution were read and then key words were extracted which seemed to occur more at one end of the distribution than the other\n",
    "- Machine learning: machine learning techniques were used with features from Key Words and other data extracted from the text\n",
    "- Deep learning: deep learning techniques were used. This is the most refined code and the place where experienced data scientists would find most value in reviewing\n",
    "\n",
    "The winning submission resulted from combining the methods.\n",
    "\n",
    "Note on the code contained in this notebook:\n",
    "- We removed most of the exploratory code from this notebook to focus on what we actually used in the final predictions. Some irrelevant and duplicte elements remain. This code was written by different people with different levels of coding expertise. Thus, the application of code can vary widely and may seem disjointed/incoherent at times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "- pandas (https://pandas.pydata.org/)\n",
    "- numpy (http://www.numpy.org/)\n",
    "- seaborn (https://seaborn.pydata.org/)\n",
    "- scikit-learn (https://scikit-learn.org/)\n",
    "- scipy (https://www.scipy.org/)\n",
    "- pyspellchecker (https://github.com/barrust/pyspellchecker)\n",
    "- textblob (https://textblob.readthedocs.io/en/dev/)\n",
    "- spacy (https://spacy.io/)\n",
    "- tpot (http://epistasislab.github.io/tpot/)\n",
    "- xgboost (https://xgboost.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m0a00q3\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\deap\\tools\\_hypervolume\\pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.\n",
      "  \"module. Expect this to be very slow.\", ImportWarning)\n"
     ]
    }
   ],
   "source": [
    "# Python's best-known DataFrame implementation\n",
    "import pandas as pd\n",
    "\n",
    "# Fast, flexible array and numerical linear algebra subroutines\n",
    "import numpy as np\n",
    "\n",
    "# OS utilities (e.g. path module)\n",
    "import os\n",
    "\n",
    "# Plots & other visualization\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pretty printing of complex datatypes\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "# Preprocessing and modeling utilities\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import SparsePCA, TruncatedSVD\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, BayesianRidge\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Evaluation\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Text processing tools\n",
    "from spellchecker import SpellChecker\n",
    "from textblob import TextBlob\n",
    "\n",
    "import language_check\n",
    "from textstat.textstat import textstatistics, easy_word_set, legacy_round \n",
    "\n",
    "\n",
    "# For word embeddings and syntactic features\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# AutoML\n",
    "from tpot import TPOTRegressor\n",
    "\n",
    "#xgboost\n",
    "from xgboost import XGBRegressor\n",
    "import scipy\n",
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Here we set some constant values related to local paths to data files as well as lists containing the various predictor and target features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to various data targets for the competition. \n",
    "\n",
    "# Update to reflect the directory hierarchy of your machine \n",
    "DATA_DIR = \"C:\\\\Users\\\\m0a00q3\\\\OneDrive - Walmart Inc\\\\SIOP 2019 - NLP Challenge\\\\Data\"\n",
    "\n",
    "TRAIN_CSV_DATA_NAME = \"siop_ml_train_participant.csv\"\n",
    "TEST_CSV_DATA_NAME = \"siop_ml_dev_participant.csv\"\n",
    "FINAL_CSV_DATA_NAME = \"siop_ml_test_participant.csv\"\n",
    "\n",
    "# Set some DataFrame-specific constants\n",
    "TARGET_COLUMN_NAMES = [attribute + \"_Scale_score\" for attribute in [\"A\", \"E\", \"O\", \"N\", \"C\"]]\n",
    "PREDICTOR_TEXT_COLUMN_NAMES = [\"open_ended_\" + str(idx) for idx in range(1, 6)]\n",
    "PREDICTOR_CONCAT_COLUMN_NAME = \"open_ended_6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading In Data\n",
    "\n",
    "Here we use `pandas` to read our csv data sets into a DataFrame, a common and convenient data structure for the workflows we will be implementing. `df_train` will be used for training purposes, `df_test` will be used for public leaderboard submissions, and 'df_train' will be used for the private leaderboards submissions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    1088\n",
       "Test      300\n",
       "Final     300\n",
       "Name: Source, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv data to base DataFrame\n",
    "df_train_temp = pd.read_csv(os.path.join(DATA_DIR, TRAIN_CSV_DATA_NAME))\n",
    "df_test_temp = pd.read_csv(os.path.join(DATA_DIR, TEST_CSV_DATA_NAME))\n",
    "df_final_temp = pd.read_csv(os.path.join(DATA_DIR, FINAL_CSV_DATA_NAME))\n",
    "\n",
    "df_train_temp['Source']='Train'\n",
    "df_test_temp['Source']='Test'\n",
    "df_final_temp['Source']='Final'\n",
    "\n",
    "# Combine datasets datasets\n",
    "df_total=pd.concat([df_train_temp,df_test_temp,df_final_temp],ignore_index=True, sort=True)\n",
    "\n",
    "# Check data load\n",
    "df_total['Source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Modules\n",
    "\n",
    "Here we define various preprocessing utilities (simple python functions that operate on a single input) as well as preprocessing transformers which operate on an entire column of data. Transformers should be implemented as Python classes that inherit from `sklearn.base.BaseEstimator` and `sklearn.base.TransformerMixin` & should implement a `fit` and `transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[PREDICTOR_CONCAT_COLUMN_NAME] = df_total.apply(\n",
    "    lambda row: \" \".join([row[col_name] for col_name in PREDICTOR_TEXT_COLUMN_NAMES]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "PREDICTOR_TEXT_COLUMN_NAMES_ALL =['open_ended_1','open_ended_2','open_ended_3',\n",
    "                                  'open_ended_4','open_ended_5','open_ended_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and Correct Spelling Errors. \n",
    "\n",
    "spell_checker = SpellChecker()\n",
    "\n",
    "def tokenize(text):\n",
    "    return TextBlob(text).words\n",
    "\n",
    "def compute_num_spelling_errors(text):\n",
    "    return len(spell_checker.unknown(tokenize(text)))\n",
    "\n",
    "def divide(x, y):\n",
    "    return x / y\n",
    "\n",
    "def word_count(text): \n",
    "    return textstatistics().lexicon_count(text, removepunct=True)\n",
    "\n",
    "for predictor_col in PREDICTOR_TEXT_COLUMN_NAMES_ALL:\n",
    "    df_total[predictor_col + \"_num_words\"] = df_total[predictor_col].apply(word_count)\n",
    "    df_total[predictor_col + \"_num_misspelled\"] = df_total[predictor_col].apply(compute_num_spelling_errors)\n",
    "    df_total[predictor_col + \"_percent_misspelled\"] = df_total[[predictor_col + \"_num_misspelled\",\n",
    "                              predictor_col + \"_num_words\"\n",
    "    ]].apply(lambda x: divide(*x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building word lists 1\n",
    "\n",
    "We build the word lists twice because we were lazy. The lists diverged due to different team members refining them and we never got around to reconciling the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists were compliled by reading a sample of comments at either the top or bottom 5% of each trait distribution\n",
    "\n",
    "O_high_5_LIST = [\"accept\",\"allow\",\"apply\",\"benefit\",\"better\",\"career\",\"client\",\"comfortable\",\"contact\",\"contribute\",\"convince\",\n",
    "\"correct\",\"enjoy\",\"excited\",\"fair\",\"first\",\"fun\",\"great time\",\"grow\",\"happy to go\",\"help\",\"immediately\",\"improve\",\"insist\",\n",
    "\"leader\",\"learn\",\"let\",\"mad\",\"negative\",\"no problem\",\"offer\",\"personal issue\",\"respect\",\"right away\",\"show\",\"team\"]\n",
    "\n",
    "C_high_2_LIST = [\"family\",\"report\",\"stress\",\"question\",\"convince\",\"job\",\"deserve\",\"longest\",\"comfortable\",\"win\",\"great time\",\n",
    "\"negative\",\"fair\",\"check in\",\"short time\",\"accus\",\"short \",\"respect\",\"willing\",\"lie\",\"correct\",\"as soon\",\"positive\",\"impres\",\n",
    "\"review\",\"problems\",\"immediately\",\"hate networking\",\"anger\",\"proof\",\"upset\",\"prove\",\"open\",\"explain\",\"improve\",\"time \",\n",
    "\"confident\",\"right away\",\"let\"]\n",
    "\n",
    "A_high_1_LIST = [\"agree\",\"benefit\",\"best\",\"bonus\",\"change\",\"compromise\",\"considerate\",\"correct\",\"defer\",\"easy going\",\"family\",\n",
    "\"flexible\",\"fun\",\"good\",\"help\",\"hurt\",\"incorrect\",\"leader\",\"let\",\"misunderstand\",\"no problem\",\"not interested\",\"obligation\",\n",
    "\"paid\",\"pick\",\"priority\",\"problems\",\"quickly\",\"respect\",\"review\",\"show\",\"willing\",\"win\"]\n",
    "\n",
    "E_high_3_LIST = [\"career\",\"good\",\"frustrated\",\"nice\",\"best\",\"deny\",\"reflect\",\"confident\",\"grow\",\"consequence\",\"missed out\",\n",
    "\"connect\",\"rage\",\"importan\",\"worry\",\"I am sociable\",\"party\",\"right away\",\"priority\",\"sociable\",\"accept\",\"focus\",\"plan \",\n",
    "\"report\",\"excited\",\"reward\",\"contribute\",\"allow\",\"success\",\"contact\",\"review\",\"absolutely go\",\"for sure go\",\"meet\",\"great\",\n",
    "\"colleagues\",\"social\",\"not need anyone\",\"regardless\",\"fool\",\"surely attend\",\"leader\",\"network\",\"I like parties\",\"no problem\",\n",
    "\"learn\",\"friendship\",\"definitely go\",\"introduce\",\"let\",\"competition\",\"client\", \"make new friends\"]\n",
    "\n",
    "A_low_2_LIST = [\"wrong\",\"question\",\"busy\",\"probably\",\"resent\",\"not go\",\"importan\",\"fun\",\"enjoy\",\"bad\",\"first\",\"problems\",\n",
    "\"refuse\",\"better\",\"short time\",\"good\",\"anxiety\",\"avoid going\",\"respect\",\"compromise\",\"losing\",\"angry\",\"regardless\",\n",
    "\"social anxiety\",\"rage\",\"decline\",\"pretend\",\"focus\",\"connect\",\"no problem\",\"priority\",\"excuse\",\"procrastinate\",\"fool\",\"sick\",\n",
    "\"personal issue\",\"anticipat\",\"deadline\",\"anyone\",\"lose\",\"difficult\",\"meet\",\"judg\",\"worry\",\"plan \",\"trouble\",\"show\",\"nervous\",\n",
    "\"reflect\",\"help\",\"pressure\",\"compensate\",\"bonus\",\"get along\",\"flexible\",\"colleagues\",\"accus\",\"fire\",\"consequence\",\"demand\",\n",
    "\"not back down\",\"stand my ground\"]\n",
    "            \n",
    "A_low_3_LIST = [\"I like parties\",\"fire\",\"unpleasant\",\"would not go\",\"sales\",\"quit\",\"discomfort\",\"money\",\"hate networking\",\n",
    "\"worthwhile\",\"obligation\",\"panic\",\"emotion\",\"unlikely\",\"hell\",\"skip\",\"social anxious\",\"pick\",\"cold\",\"decline\",\"not go\",\"paid\",\n",
    "\"get out of it\",\"hate\",\"wouldn't go\",\"reward\",\"rage\",\"short \",\"negotiate\",\"beg\",\"difficult\",\"trouble\",\"resent\",\"time \",\n",
    "\"immediately\",\"stress\",\"stressed\",\"stressed out\",\"reconsider\",\"short time\",\"grow\",\"extremely uncomfortable\",\"willing\",\n",
    "\"get along\",\"apply\",\"if i had to\",\"risk\",\"anxiety\",\"great\",\"forc\",\"allow\",\"socially awkward\",\"dislike\",\"I am sociable\",\n",
    "\"great time\",\"missed out\",\"compensate\",\"oppurtunity\",\"anger\",\"benefit\",\"plan \",\"confirm\",\"avoid\",\"social anxiety\",\"fair\",\n",
    "\"pressure\",\"mad\",\"deserve\",\"not a social person\"]\n",
    "A_low_4_LIST = [\"rage\",\"cold\",\"fool\",\"marked\",\"depend\",\"demand\",\"quit\",\"report\",\"probably\",\"career\",\"accept\",\"not go\",\n",
    "\"compensate\",\"pressure\",\"quiet\",\"angry\",\"afraid\",\"confront\",\"emotion\",\"job\",\"benefit\",\"mad\",\"threaten\",\"money\",\"unpleasant\",\n",
    "\"anxiety\",\"pissed\",\"anyone\",\"obligation\",\"confident\",\"short \",\"regardless\",\"refuse\",\"appeal\",\"hesitate\",\"examples\",\"immediately\",\n",
    "\"bad\",\"suck it up\",\"resent\",\"respect\",\"wrong\",\"harm\"]\n",
    "A_low_5_LIST = [\"paid\",\"refuse\",\"avoid going\",\"alone\",\"emotion\",\"pretend\",\"resent\",\"bonus\",\"win\",\"rage\",\"difficult\",\"probably\",\n",
    "\"afraid\",\"anger\",\"forc\",\"hate networking\",\"change\",\"agree\",\"depend\",\"wouldn't go\",\"pick\",\"focus\",\"obligation\",\"frustrated\",\n",
    "\"considerate\",\"right away\",\"time \",\"money\",\"negative\",\"colleagues\",\"awkward\",\"improve\",\"success\",\"explain\",\"bad\",\"best\",\n",
    "\"respect\",\"let\",\"better\",\"nice\",\"nervous\"]\n",
    "\n",
    "N_low_1_LIST = [\"as soon\",\"report\",\"show\",\"problems\",\"best\",\"quickly\",\"bonus\",\"tense\",\"social\",\"correct\",\"win\",\"concede\",\n",
    "\"leader\",\"misunderstand\",\"unlikely\",\"incorrect\",\"fire\",\"easy going\",\"paid\",\"hesitate\",\"human resources\",\"time \",\"emotion\",\n",
    "\"worried\",\"racist\",\"slash\",\"fun\",\"valid\",\"stubborn\",\"flexible\",\"review\",\"beg\",\"respect\",\"benefit\",\"open\",\"threaten\",\"short \",\n",
    "\"change\",\"first\",\"trouble\",\"agree\",\"compromise\",\"defend\",\"defer\",\"mad\",\"harm\",\"worry\"]\n",
    "N_low_2_LIST = [\"hate networking\",\"client\",\"responsible\",\"longest\",\"unhappy\",\"willing\",\"accus\",\"proof\",\"difficult\",\"family\",\n",
    "\"anger\",\"team\",\"correct\",\"consequence\",\"comfortable\",\"stick\",\"trouble\",\"job\",\"pressure\",\"benefit\",\"mad\",\"report\",\"deserve\",\n",
    "\"accept\",\"positive\",\"review\",\"open\",\"as soon\",\"risk\",\"time \",\"let\",\"feel pressure\",\"check in\",\"depend\",\"dislike\",\"judg\",\n",
    "\"social anxiety\",\"resent\",\"lie\",\"explain\",\"upset\",\"hard \",\"leader\",\"frustrated\"]\n",
    "N_low_3_LIST = [\"learn\",\"no problem\",\"regardless\",\"network\",\"good\",\"introduce\",\"anyone\",\"definitely go\",\"confident\",\"meet\",\n",
    "\"competition\",\"contact\",\"lie\",\"client\",\"I like parties\",\"not need anyone\",\"great\",\"social\",\"party\",\"worry\",\"friendship\",\n",
    "\"review\",\"contribute\",\"stretch myself\",\"surely attend\",\"fool\",\"plan \",\"help\",\"leader\",\"missed out\",\"for sure go\",\"fair\",\n",
    "\"let\",\"reluctance\",\"absolutely go\",\"excited\",\"happy to go\",\"priority\",\"excuse\",\"hard \",\"report\",\"job\",\"anger\"]\n",
    "N_low_5_LIST = [\"learn\",\"anyone\",\"short time\",\"help\",\"leader\",\"client\",\"great time\",\"enjoy\",\"importan\",\"excited\",\"hesitate\",\n",
    "\"correct\",\"lie\",\"team\",\"losing\",\"career\",\"responsible\",\"insist\",\"immediately\",\"bad\",\"happy to go\",\"pretend\",\"willing\",\n",
    "\"emotion\",\"short \",\"stress\",\"confus\",\"trouble\",\"time \",\"worry\",\"success\",\"regardless\",\"report\",\"hurt\",\"show\",\"money\",\n",
    "\"contact\",\"stick\",\"mad\",\"unlikely\"]\n",
    "\n",
    "C_high_1_LIST = [\"question\",\"stubborn\",\"would change\",\"reconsider\",\"as soon\",\"human resources\",\"disagree\",\"defer\",\"risk\",\n",
    "\"unpleasant\",\"immediately\",\"worry\",\"argue\",\"petty\",\"explain\",\"mad\",\"proof\",\"hurt\",\"correct\",\"obligated\",\"not go\",\"harm\",\n",
    "\"unhappy\",\"leader\",\"misunderstand\",\"win\",\"fire\",\"unlikely\",\"first\",\"pick\",\"angry\",\"priority\",\"bonus\",\"quickly\",\"short time\",\n",
    "\"hesitate\",\"tense\",\"social\",\"switch\",\"success\",\"problems\",\"not interested\",\"easy going\",\"no problem\",\"report\",\"reflect\",\n",
    "\"upset\",\"anger\",\"team\",\"valid\",\"paid\",\"review\",\"agree\",\"short \",\"willing\",\"fun\",\"concede\",\"show\",\"seniority\",\"flexible\",\n",
    "\"change\"]\n",
    "C_high_3_LIST = [\"no problem\",\"introvert\",\"frustrated\",\"win\",\"sad\",\"missed out\",\"alone\",\"dislike\",\"appeal\",\"depend\",\"insist\",\n",
    "\"sick\",\"success\",\"not comfortable\",\"stretch myself\",\"report\",\"benefit\",\"accept\",\"hard \",\"contribute\",\"responsible\",\"compensate\",\n",
    "\"fool\",\"social\",\"absolutely go\",\"regardless\",\"anyone\",\"focus\",\"pretend\",\"worried\",\"nightmare\",\"not need anyone\",\"surely attend\",\n",
    "\"for sure go\",\"colleagues\",\"competition\",\"let\",\"help\",\"best\",\"great\",\"deny\",\"importan\",\"learn\",\"network\",\"client\",\"uncomfortable\",\n",
    "\"priority\",\"lie\",\"improve\",\"good\",\"not attend\",\"fun\",\"definitely go\",\"mad\",\"comfortable\",\"reluctant\",\"excited\",\"excuse\",\"meet\"]\n",
    "C_high_4_LIST = [\"willing\",\"change\",\"respect\",\"connect\",\"fun\",\"paid\",\"hard \",\"immediately\",\"terrible\",\"grow\",\"incorrect\",\"refuse\",\n",
    "\"open\",\"resent\",\"quickly\",\"contact\",\"calm\",\"party\",\"short \",\"contribute\",\"my right\",\"stubborn\",\"rebut\",\"problems\",\"worried\",\n",
    "\"as soon\",\"compromise\",\"hurt\",\"good\",\"proof\",\"not true\",\"early\",\"human resources\"\"obligation\",\"colleagues\",\"meet\",\"demand\",\n",
    "\"success\",\"negative\",\"allow\",\"concerned\",\"disagree\",\"let\",\"agree\"]\n",
    "C_high_5_LIST = [\"success\",\"appeal\",\"worry\",\"fun\",\"busy\",\"hesitate\",\"problems\",\"allow\",\"hurt\",\"improve\",\"excited\",\"good\",\"bad\",\n",
    "\"leader\",\"stress\",\"importan\",\"excuse\",\"introduce\",\"lose\",\"enjoy\",\"prove\",\"personal issue\",\"fair\",\"quickly\",\"correct\",\"stick\",\n",
    "\"accus\",\"unlikely\",\"comfortable\",\"sad\",\"willing\",\"contact\",\"confus\",\"career\",\"show\",\"losing\",\"immediately\",\"compensate\",\n",
    "\"anyone\",\"lie\",\"client\",\"help\",\"learn\"]\n",
    "\n",
    "A_high_2_LIST = [\"agree\",\"negative\",\"benefit\",\"overwhelmed\",\"quiet\",\"I had to\",\"lie\",\"team\",\"check in\",\"early\",\"stick\",\n",
    "\"feel pressure\",\"allow\",\"family\",\"sacrifice\",\"stressed\",\"learn\",\"frustrated\",\"right away\",\"convince\",\"best\",\"let\",\"fair\",\n",
    "\"client\",\"longest\",\"responsible\",\"mad\",\"stressed out\",\"report\",\"time \",\"upset\",\"confident\",\"dislike\",\"unhappy\",\"anger\",\n",
    "\"explain\",\"positive\",\"stress\",\"proof\",\"avoid confrontation\",\"more than willing\",\"don't want conflict\",\"easy going\",\n",
    "\"hate conflict\",\"keep people happy\",\"team player\"]\n",
    "A_high_3_LIST = [\"change\",\"reluctant\",\"angry\",\"quickly\",\"right away\",\"excuse\",\"stick\",\"would change\",\"early\",\"compromise\",\n",
    "\"not comfortable\",\"learn\",\"positive\",\"avoid going\",\"anxious\",\"colleagues\",\"fool\",\"reluctance\",\"absolutely go\",\"fun\",\n",
    "\"not attend\",\"tired\",\"losing\",\"worry\",\"busy\",\"no problem\",\"contribute\",\"explain\",\"hurt\",\"network\",\"uncomfortable\",\"consequence\",\n",
    "\"social\",\"not need anyone\",\"surely attend\",\"regardless\",\"help\",\"better\",\"excited\",\"importan\",\"priority\",\"responsible\",\n",
    "\"outside of my comfort zone\",\"party\",\"stretch myself\",\"for sure go\",\"hard \",\"report\",\"focus\",\"client\",\"alone\",\"lie\",\"introduce\",\n",
    "\"friendship\",\"comfortable\",\"contact\",\"best\",\"good\",\"definitely go\",\"anyone\",\"meet\"]\n",
    "A_high_4_LIST = [\"convince\",\"defend\",\"lose\",\"accus\",\"worthwhile\",\"agitated\",\"personal\",\"consequence\",\"concerned\",\"impres\",\n",
    "\"anger\",\"success\",\"correct\",\"win\",\"confus\",\"argue\",\"proof\",\"incorrect\",\"focus\",\"terrible\",\"best\",\"negative\",\"not justified\",\n",
    "\"as soon\",\"plead\",\"confirm\",\"lie\",\"unfair\",\"early\",\"judg\",\"stressed out\",\"hard \",\"organize\",\"risk\",\"improve\",\"worried\",\"quickly\",\n",
    "\"my right\",\"open\",\"frustrated\",\"contact\",\"meet\",\"compromise\",\"pretend\",\"rebut\",\"stress\",\"reconsider\",\"hurt\",\"would not go\",\n",
    "\"importan\",\"positive\",\"problems\",\"agree\",\"let\",\"negotiate\",\"allow\",\"explain\",\"learn\",\"prove\",\"better\",\"anxious\",\"colleagues\",\n",
    "\"not true\",\"upset\",\"grow\"]\n",
    "A_high_5_LIST = [\"accept\",\"short time\",\"question\",\"happy to go\",\"excited\",\"hard \",\"impres\",\"good\",\"grow\",\"losing\",\"reward\",\n",
    "\"show\",\"contribute\",\"convince\",\"accus\",\"willing\",\"concerned\",\"dislike\",\"contact\",\"hesitate\",\"network\",\"comfortable\",\"apply\",\n",
    "\"leader\",\"immediately\",\"stress\",\"correct\",\"importan\",\"great time\",\"hurt\",\"offer\",\"confus\",\"help\",\"anyone\",\"lie\",\"client\",\n",
    "\"enjoy\",\"learn\"]\n",
    "\n",
    "N_high_1_LIST = [\"willing\",\"regardless\",\"losing\",\"great\",\"shy\",\"career\",\"obligated\",\"organize\",\"stick\",\"forc\",\"appeal\",\"anger\",\n",
    "\"unfair\",\"positive\",\"early\",\"reward\",\"my right\",\"I had to\",\"refuse\",\"money\",\"negotiate\",\"personal issue\",\"wrong\",\"anyone\",\n",
    "\"family\",\"enjoy\",\"pissed\",\"hard \",\"team\",\"deny\",\"insist\",\"busy\",\"sacrifice\",\"skip\",\"proof\",\"fair\",\"client\",\"better\",\"contact\",\n",
    "\"meet\",\"question\",\"fool\",\"get even\",\"profanity\",\"cold\",\"unhappy\",\"angry\",\"call in\",\"awkward\",\"excuse\",\"upset\",\"get along\",\n",
    "\"demand\",\"lose\",\"avoid\",\"deadline\",\"stressed\",\"unpleasant\",\"terrible\",\"difficult\",\"frustrated\",\"confront\",\"hell\",\"plead\",\n",
    "\"alone\",\"improve\",\"stare\",\"concerned\",\"hardship\",\"nice\",\"pressure\",\"sad\",\"reflect\",\"probably\",\"friendship\",\"reluctant\",\n",
    "\"sick\",\"obligation\",\"quit\",\"hate\",\"offer\",\"hard stance\"]\n",
    "N_high_2_LIST = [\"calm\",\"panic\",\"stress\",\"enjoy\",\"bonus\",\"show\",\"learn\",\"question\",\"decline\",\"sick\",\"importan\",\"colleagues\",\n",
    "\"worried\",\"worry\",\"connect\",\"meet\",\"rage\",\"paid\",\"pretend\",\"anxiety\",\"avoid going\",\"lose\",\"early\",\"bad\",\"angry\",\"better\",\n",
    "\"deadline\",\"losing\",\"hurt\",\"priority\",\"no problem\",\"wrong\",\"demand\",\"beg\",\"I had to\",\"busy\",\"compromise\",\"negotiate\",\"probably\"]\n",
    "N_high_3_LIST = [\"wrong\",\"compromise\",\"respect\",\"risk\",\"show\",\"afraid\",\"bonus\",\"worried\",\"tense\",\"worthwhile\",\"dislike\",\"valid\",\n",
    "\"confirm\",\"socially awkward\",\"introvert\",\"losing\",\"deserve\",\"quickly\",\"beg\",\"plead\",\"mad\",\"change\",\"better\",\"angry\",\"apply\",\n",
    "\"not comfortable\",\"lose\",\"get out of it\",\"agree\",\"paid\",\"outside of my comfort zone\",\"not a social person\",\"miss out\",\n",
    "\"time \",\"family\",\"reconsider\",\"anxious\",\"short time\",\"prove\",\"negative\",\"money\",\"fire\",\"tired\",\"negotiate\",\"I had to\",\"harm\",\n",
    "\"appeal\",\"sacrifice\",\"hell\",\"stress\",\"awkward\",\"forc\",\"hesitate\",\"pressure\",\"trouble\",\"willing\",\"deadline\",\"short \",\"suck it up\",\n",
    "\"get along\",\"loner\",\"stressed\",\"resent\",\"skip\",\"social anxiety\",\"bad\",\"not great at networking\",\"nightmare\",\"shy\",\"avoid\",\n",
    "\"impres\",\"concerned\",\"difficult\",\"probably\",\"compensate\",\"emotion\",\"unpleasant\",\"obligation\",\"nervous\",\"feel pressure\",\n",
    "\"extremely uncomfortable\",\"nerve-wracking\",\"hate networking\",\"immediately\",\"hate\",\"would not go\",\"social anxious\",\"panic\",\n",
    "\"unlikely\",\"discomfort\",\"not go\",\"anxiety\"]\n",
    "N_high_5_LIST = [\"negative\",\"allow\",\"best\",\"hate networking\",\"let\",\"positive\",\"apply\",\"anger\",\"beg\",\"bonus\",\"comfortable\",\n",
    "\"dislike\",\"oppurtunity\",\"obligation\",\"improve\",\"concerned\",\"pick\",\"open\",\"right away\",\"job\",\"rage\",\"probably\",\"refuse\",\n",
    "\"upset\",\"afraid\",\"risk\",\"alone\",\"social anxiety\",\"consequence\",\"agree\",\"prove\",\"fair\",\"colleagues\",\"awkward\",\"paid\",\"grow\",\n",
    "\"avoid going\",\"early\",\"nervous\",\"forc\",\"depend\",\"resent\",\"frustrated\",\"difficult\"]\n",
    "\n",
    "O_high_1_LIST = [\"accept\",\"anyone\",\"as soon\",\"best\",\"bonus\",\"defer\",\"easy going\",\"enjoy\",\"excuse\",\"flexible\",\"fool\",\"get even\",\n",
    "\"good\",\"harm\",\"hesitate\",\"hurt\",\"importan\",\"leader\",\"marked\",\"meet\",\"negotiate\",\"obligation\",\"paid\",\"petty\",\"pick\",\"plead\",\n",
    "\"positive\",\"probably\",\"problems\",\"quickly\",\"quit\",\"reflect\",\"respect\",\"reward\",\"short \",\"short time\",\"stubborn\",\"suck it up\",\n",
    "\"suffer\",\"tense\",\"terrible\",\"threaten\",\"time \",\"upset\",\"willing\",\"win\",\"worried\"]\n",
    "O_high_2_LIST = [\"agree\",\"allow\",\"anger\",\"best\",\"better\",\"calm\",\"correct\",\"deserve\",\"difficult\",\"excuse\",\"explain\",\"fair\",\n",
    "\"forc\",\"frustrated\",\"fun\",\"great time\",\"immediately\",\"importan\",\"improve\",\"learn\",\"let\",\"nervous\",\"offer\",\"pick\",\"positive\",\n",
    "\"pressure\",\"problems\",\"proof\",\"prove\",\"respect\",\"responsible\",\"review\",\"short \",\"short time\",\"show\",\"suffer\",\"team\",\"time \",\n",
    "\"trouble\",\"upset\",\"worried\"]\n",
    "O_high_3_LIST = [\"absolutely go\",\"accept\",\"alone\",\"angry\",\"anyone\",\"better\",\"career\",\"client\",\"cold\",\"comfortable\",\"consequence\",\n",
    "\"contact\",\"contribute\",\"definitely go\",\"deny\",\"depend\",\"difficult\",\"early\",\"emotion\",\"excited\",\"excuse\",\"feel pressure\",\"focus\",\n",
    "\"for sure go\",\"forc\",\"friendship\",\"fun\",\"good\",\"hesitate\",\"I like parties\",\"insist\",\"introduce\",\"let\",\"lie\",\"lose\",\"losing\",\n",
    "\"meet\",\"miss out\",\"missed out\",\"money\",\"nerve-wracking\",\"nervous\",\"network\",\"nice\",\"no problem\",\"not comfortable\",\"not need anyone\",\n",
    "\"obligated\",\"oppurtunity\",\"outside of my comfort zone\",\"party\",\"plan \",\"positive\",\"priority\",\"quickly\",\"regardless\",\"responsible\",\n",
    "\"success\",\"trouble\",\"worried\",\"worry\",\"would change\"]\n",
    "O_high_4_LIST = [\"allow\",\"anxious\",\"as soon\",\"benefit\",\"best\",\"better\",\"bonus\",\"client\",\"cold\",\"colleagues\",\"comfortable\",\n",
    "\"concerned\",\"confirm\",\"connect\",\"consequence\",\"deserve\",\"early\",\"explain\",\"fool\",\"forc\",\"fun\",\"great time\",\"grow\",\"help\",\n",
    "\"importan\",\"impres\",\"improve\",\"judg\",\"learn\",\"let\",\"lie\",\"losing\",\"marked\",\"meet\",\"negotiate\",\"nervous\",\"nice\",\"not justified\",\n",
    "\"not true\",\"offer\",\"paid\",\"party\",\"personal\",\"personal issue\",\"plan \",\"positive\",\"pretend\",\"problems\",\"prove\",\"quiet\",\n",
    "\"reconsider\",\"resent\",\"respect\",\"review\",\"risk\",\"stick\",\"stubborn\",\"team\",\"threaten\",\"trouble\"]\n",
    "\n",
    "A_low_1_LIST = [\"stare\",\"responsible\",\"fool\",\"get even\",\"profanity\",\"call in\",\"sick\",\"refuse\",\"emotion\",\"hard stance\",\"racist\",\n",
    "\"slash\",\"hardship\",\"demand\",\"compensate\",\"first\",\"stick\",\"quit\",\"personal issue\",\"excuse\",\"trouble\",\"deny\",\"hell\",\"depend\",\n",
    "\"money\",\"cold\",\"hard \",\"marked\",\"pissed\",\"client\",\"deserve\",\"unfair\",\"fair\",\"resent\",\"reconsider\",\"offer\",\"my right\",\"hate\",\n",
    "\"forc\",\"worry\",\"reward\",\"reluctant\",\"concerned\",\"organize\",\"sad\",\"losing\",\"rage\",\"bad\",\"insist\",\"busy\",\"difficult\",\"appeal\",\n",
    "\"stressed out\",\"stressed\",\"wrong\",\"early\",\"longest\",\"proof\",\"better\",\"petty\",\"improve\",\"contact\",\"avoid\",\"accept\",\"entitle\",\n",
    "\"meet\",\"if i had to\",\"seniority\",\"suffer\",\"comfortable\",\"regardless\",\"personal\"]\n",
    "\n",
    "E_low_3_LIST = [\"social anxiety\",\"extremely uncomfortable\",\"nervous\",\"social anxious\",\"panic\",\"unlikely\",\"impres\",\"anxiety\",\n",
    "\"probably\",\"introvert\",\"immediately\",\"feel pressure\",\"anxious\",\"decline\",\"emotion\",\"nerve-wracking\",\"loner\",\"pressure\",\"avoid\",\n",
    "\"stressed out\",\"stressed\",\"dislike\",\"shy\",\"hesitate\",\"losing\",\"bad\",\"difficult\",\"not great at networking\",\"obligation\",\"unfair\",\n",
    "\"stretch myself\",\"hate networking\",\"willing\",\"hell\",\"stress\",\"lose\",\"nightmare\",\"quit\",\"avoid going\",\"mad\",\"paid\",\"sad\",\n",
    "\"reluctant\",\"get out of it\",\"fair\",\"not a social person\",\"reluctance\",\"quiet\",\"upset\",\"sacrifice\",\"change\",\"not comfortable\",\n",
    "\"money\",\"tired\",\"family\",\"appeal\",\"confirm\",\"harm\",\"prove\",\"short \",\"skip\",\"stick\",\"hate\",\"compensate\",\"deserve\",\"short time\",\n",
    "\"sick\",\"outside of my comfort zone\",\"deadline\",\"pretend\",\"discomfort\",\"socially awkward\",\"show\",\"angry\",\"win\",\"convince\",\n",
    "\"not interested\",\"apply\",\"get along\",\"negotiate\",\"unpleasant\",\"quickly\",\"awkward\",\"not attend\",\"concerned\",\"plead\",\"fire\",\n",
    "\"suck it up\",\"forc\",\"comfortable\",\"pick\",\"uncomfortable\",\"unhappy\",\"excuse\",\"compromise\",\"afraid\",\"do not interact well with strangers\",\n",
    "\"don't like being in social situations\",\"don't like networking\",\"don't like socializing\",\"very shy\"]\n",
    "\n",
    "GO_3_LIST = [\"absolutely go\",\"all in\",\"attend\",\"attend that meeting\",\"certainly go\",\"cheerfully go\",\"decide to go\",\n",
    "\"definitely attend\",\"definitely be in attendance_1\",\"definitely go\",\"definitely still go\",\"go for it\",\"go for sure\",\n",
    "\"go to the event\",\"go to the meeting\",\"go to the networking meeting\",\"just go\",\"make an appearance\",\"make sure I go\",\n",
    "\"make time to attend\",\"still attend\",\"still go\",\"still opt in\",\"time and go\",\"would attend\",\"would go\",\"would still go\"]\n",
    "\n",
    "NOGO_3_LIST = [\"avoid\",\"backing out\",\"bow out of the meeting\",\"choose not to go\",\"come\",\"consider not going\",\"decide to go\",\n",
    "\"decline\",\"ditch\",\"get out of it\",\"go home\",\"happy to go\",\"hate going\",\"hesitate to go\",\"in attendance\",\"likely go\",\n",
    "\"likely not go\",\"might consider going\",\"no interest\",\"not attend\",\"not come\",\"not consider going\",\"not feel like going\",\n",
    "\"not going\",\"not interested\",\"not show up\",\"not volunteer\",\"not want to go\",\"politely decline\",\"probably attend\",\"probably go\",\n",
    "\"probably not go\",\"probably still go\",\"probably would not\",\"probably wouldn't\",\"skip\",\"stay at home\",\"try to go\",\"unlikely to go\",\n",
    "\"will not go\",\"would be going\",\"would not go\",\"wouldn't be going\",\"wouldn't do it\",\"wouldn't go\",\"wouldn't want to go\"]\n",
    "\n",
    "GO_5_LIST = ['would go','probably go']\n",
    "\n",
    "NOGO_5_LIST = ['not go','not to go',\"n't go\"]\n",
    "\n",
    "NOT_LIST = [\" not \"]\n",
    "\n",
    "NO_LIST = [\" no \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for counting word occurance\n",
    "\n",
    "def write_keyword_count_column(df, target_column, source_column, keyword_list):\n",
    "    def compute_keyword_list_count(text):\n",
    "        return sum([text.count(kw) for kw in keyword_list])    \n",
    "    df[target_column] = df[source_column].apply(compute_keyword_list_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify key word list features\n",
    "\n",
    "write_keyword_count_column(df_total, 'O_high_5', 'open_ended_5', O_high_5_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'C_high_2', 'open_ended_2', C_high_2_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'A_high_1', 'open_ended_1', A_high_1_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'E_high_3', 'open_ended_3', E_high_3_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'A_low_2', 'open_ended_2', A_low_2_LIST)\n",
    "write_keyword_count_column(df_total, 'A_low_3', 'open_ended_3', A_low_3_LIST)\n",
    "write_keyword_count_column(df_total, 'A_low_4', 'open_ended_4', A_low_4_LIST)\n",
    "write_keyword_count_column(df_total, 'A_low_5', 'open_ended_5', A_low_5_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'N_low_1', 'open_ended_1', N_low_1_LIST)\n",
    "write_keyword_count_column(df_total, 'N_low_2', 'open_ended_2', N_low_2_LIST)\n",
    "write_keyword_count_column(df_total, 'N_low_3', 'open_ended_3', N_low_3_LIST)\n",
    "write_keyword_count_column(df_total, 'N_low_5', 'open_ended_5', N_low_5_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'C_high_1', 'open_ended_1', C_high_1_LIST)\n",
    "write_keyword_count_column(df_total, 'C_high_3', 'open_ended_3', C_high_3_LIST)\n",
    "write_keyword_count_column(df_total, 'C_high_4', 'open_ended_4', C_high_4_LIST)\n",
    "write_keyword_count_column(df_total, 'C_high_5', 'open_ended_5', C_high_5_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'A_high_2', 'open_ended_1', A_high_2_LIST)\n",
    "write_keyword_count_column(df_total, 'A_high_3', 'open_ended_3', A_high_3_LIST)\n",
    "write_keyword_count_column(df_total, 'A_high_4', 'open_ended_4', A_high_4_LIST)\n",
    "write_keyword_count_column(df_total, 'A_high_5', 'open_ended_5', A_high_5_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'N_high_1', 'open_ended_1', N_high_1_LIST)\n",
    "write_keyword_count_column(df_total, 'N_high_2', 'open_ended_2', N_high_2_LIST)\n",
    "write_keyword_count_column(df_total, 'N_high_3', 'open_ended_3', N_high_3_LIST)\n",
    "write_keyword_count_column(df_total, 'N_high_5', 'open_ended_5', N_high_5_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'O_high_1', 'open_ended_1', O_high_1_LIST)\n",
    "write_keyword_count_column(df_total, 'O_high_2', 'open_ended_2', O_high_2_LIST)\n",
    "write_keyword_count_column(df_total, 'O_high_3', 'open_ended_3', O_high_3_LIST)\n",
    "write_keyword_count_column(df_total, 'O_high_4', 'open_ended_4', O_high_4_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'E_high_3', 'open_ended_3', O_high_2_LIST)\n",
    "write_keyword_count_column(df_total, 'E_high_4', 'open_ended_4', O_high_3_LIST)\n",
    "write_keyword_count_column(df_total, 'E_high_5', 'open_ended_5', O_high_4_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'A_low_1', 'open_ended_1', A_low_1_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'E_low_3', 'open_ended_3', E_low_3_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'GO_3', 'open_ended_3', GO_3_LIST)\n",
    "write_keyword_count_column(df_total, 'NOGO_3', 'open_ended_3', NOGO_3_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'GO_5', 'open_ended_5', GO_3_LIST)\n",
    "write_keyword_count_column(df_total, 'NOGO_5', 'open_ended_5', NOGO_3_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'NOT_1', 'open_ended_1', NOT_LIST)\n",
    "write_keyword_count_column(df_total, 'NOT_2', 'open_ended_2', NOT_LIST)\n",
    "write_keyword_count_column(df_total, 'NOT_3', 'open_ended_3', NOT_LIST)\n",
    "write_keyword_count_column(df_total, 'NOT_4', 'open_ended_4', NOT_LIST)\n",
    "write_keyword_count_column(df_total, 'NOT_5', 'open_ended_5', NOT_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'NO_5', 'open_ended_5', NO_LIST)\n",
    "write_keyword_count_column(df_total, 'NOT_5', 'open_ended_5', NOT_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating aggregate features--combinations were derived in part from feedback from the public leaderboard\n",
    "\n",
    "df_total['A_low_comb'] = df_total['A_low_2']+df_total['A_low_3']+df_total['A_low_4']+df_total['A_low_5']\n",
    "df_total['N_low_comb'] = df_total['N_low_1']+df_total['N_low_2']+df_total['N_low_3']+df_total['N_low_5']\n",
    "df_total['C_high_comb'] = df_total['C_high_1']+df_total['C_high_3']+df_total['C_high_4']+df_total['C_high_5']\n",
    "df_total['A_high_comb'] = df_total['A_high_2']+df_total['A_high_3']+df_total['A_high_4']+df_total['A_high_5']\n",
    "df_total['N_high_comb'] = df_total['N_high_1']+df_total['N_high_2']+df_total['N_high_3']+df_total['N_high_5']\n",
    "df_total['O_high_comb'] = df_total['O_high_1']+df_total['O_high_2']+df_total['O_high_3']+df_total['O_high_4']\n",
    "df_total['E_high_3to5'] = df_total['E_high_3']+df_total['E_high_4']+df_total['E_high_5']\n",
    "\n",
    "df_total['A_not_comb'] = df_total['NOT_1']+df_total['NOT_2']+df_total['NOT_3']+df_total['NOT_4']+df_total['NOT_5']\n",
    "\n",
    "df_total['O_go_comb'] = df_total['GO_5']-df_total['NOGO_5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building word lists 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['char_count_3'] = df_total['open_ended_3'].str.len() \n",
    "df_total['char_count_4'] = df_total['open_ended_4'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "df_total['avg_word_1'] = df_total['open_ended_1'].apply(lambda x: avg_word(x))\n",
    "df_total['avg_word_2'] = df_total['open_ended_2'].apply(lambda x: avg_word(x))\n",
    "df_total['avg_word_3'] = df_total['open_ended_3'].apply(lambda x: avg_word(x))\n",
    "df_total['avg_word_4'] = df_total['open_ended_4'].apply(lambda x: avg_word(x))\n",
    "df_total['avg_word_5'] = df_total['open_ended_5'].apply(lambda x: avg_word(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_list = [\" not \"]\n",
    "\n",
    "no_list = [\" no \"] #apply to 5 only\n",
    "\n",
    "e_high_3_list=['benefit','best','better','career','client','competition','confident','connect',\n",
    "'contact','contribute','convince','drink','enjoy','friendship','good','great','grow','importan',\n",
    "'impres','introduce','leader','learn','meet','miss out','missed out','network','open','oppurtunity',\n",
    "'party','positive','regardless','reward','sales','show','sociable','success','worthwhile']\n",
    "\n",
    "e_high_4_list=['benefit','best','better','career','client','competition','confident','connect',\n",
    "'contact','contribute','convince','drink','enjoy','friendship','good','great','grow','importan',\n",
    "'impres','introduce','leader','learn','meet','miss out','missed out','network','open','oppurtunity',\n",
    "'party','positive','regardless','reward','sales','show','sociable','success','worthwhile']\n",
    "\n",
    "e_high_5_list =['benefit','best','better','career','client','competition','confident','connect',\n",
    "'contact','contribute','convince','drink','enjoy','friendship','good','great','grow','importan',\n",
    "'impres','introduce','leader','learn','meet','miss out','missed out','network','open','oppurtunity',\n",
    "'party','positive','regardless','reward','sales','show','sociable','success','worthwhile']\n",
    "\n",
    "n_low_comb_emp_1_list=['as soon','report','show','problems','best','quickly','bonus','tense',\n",
    "'social','correct','win','concede','leader','misunderstand','unlikely','incorrect','fire',\n",
    "'easy going','paid','hesitate','human resources','time ','emotion','worried','racist','slash',\n",
    "'fun','valid','stubborn','flexible','review','beg','respect','benefit','open','threaten','short ',\n",
    "'change','first','trouble','agree','compromise','defend','defer','mad','harm']\n",
    "\n",
    "n_low_comb_emp_2_list=['worry','hate networking','client','responsible','longest','unhappy',\n",
    "'willing','accus','proof','difficult','family','anger','team','correct','consequence','comfortable',\n",
    "'stick','trouble','job','pressure','benefit','mad','report','deserve','accept','positive','review',\n",
    "'open','as soon','risk','time ','let','feel pressure','check in','depend','dislike','judg','social anxiety',\n",
    "'resent','lie','explain','upset','hard ','leader']\n",
    "\n",
    "n_low_comb_emp_3_list=['frustrated','learn','no problem','regardless','network','good','introduce',\n",
    "'anyone','definitely go','confident','meet','competition','contact','lie','client','I like parties',\n",
    "'not need anyone','great','social','party','worry','friendship','review','contribute','stretch myself',\n",
    "'surely attend','fool','plan ','help','leader','missed out','for sure go','fair','let','reluctance',\n",
    "'absolutely go','excited','happy to go','priority','excuse','hard ','report','job']\n",
    "\n",
    "n_low_comb_emp_5_list=['anger','learn','anyone','short time','help','leader','client','great time',\n",
    "'enjoy','importan','excited','hesitate','correct','lie','team','losing','career','responsible',\n",
    "'insist','immediately','bad','happy to go','pretend','willing','emotion','short ','stress','confus',\n",
    "'trouble','time ','worry','success','regardless','report','hurt','show','money','contact','stick',\n",
    "'mad','unlikely']\n",
    "\n",
    "n_high_comb_emp_1_list=['willing','regardless','losing','great','shy','career','obligated','organize',\n",
    "'stick','forc','appeal','anger','unfair','positive','early','reward','my right','I had to','refuse',\n",
    "'money','negotiate','personal issue','wrong','anyone','family','enjoy','pissed','hard ','team',\n",
    "'deny','insist','busy','sacrifice','skip','proof','fair','client','better','contact','meet','question',\n",
    "'fool','get even','profanity','cold','unhappy','angry','call in','awkward','excuse','upset','get along',\n",
    "'demand','lose','avoid','deadline','stressed','unpleasant','terrible','difficult','frustrated','confront',\n",
    "'hell','plead','alone','improve','stare','concerned','hardship','nice','pressure','sad','reflect','probably',\n",
    "'friendship','reluctant','sick','obligation','quit','hate','offer','hard stance']\n",
    "\n",
    "n_high_comb_emp_2_list=['calm','panic','stress','enjoy','bonus','show','learn','question','decline',\n",
    "'sick','importan','colleagues','worried','worry','connect','meet','rage','paid','pretend','anxiety',\n",
    "'avoid going','lose','early','bad','angry','better','deadline','losing','hurt','priority','no problem',\n",
    "'wrong','demand','beg','I had to','busy','compromise','negotiate','probably']\n",
    "\n",
    "n_high_comb_emp_3_list=['wrong','compromise','respect','risk','show','afraid','bonus','worried','tense',\n",
    "'worthwhile','dislike','valid','confirm','socially awkward','introvert','losing','deserve','quickly',\n",
    "'beg','plead','mad','change','better','angry','apply','not comfortable','lose','get out of it',\n",
    "'agree','paid','outside of my comfort zone','not a social person','miss out','time ','family','reconsider',\n",
    "'anxious','short time','prove','negative','money','fire','tired','negotiate','I had to','harm','appeal',\n",
    "'sacrifice','hell','stress','awkward','forc','hesitate','pressure','trouble','willing','deadline','short ',\n",
    "'suck it up','get along','loner','stressed','resent','skip','social anxiety','bad','not great at networking',\n",
    "'nightmare','shy','avoid','impres','concerned','difficult','probably','compensate','emotion','unpleasant',\n",
    "'obligation','nervous','feel pressure','extremely uncomfortable','nerve-wracking','hate networking','immediately',\n",
    "'hate','would not go','social anxious','panic','unlikely','discomfort','not go','anxiety']\n",
    "\n",
    "n_high_comb_emp_5_list=['negative','allow','best','hate networking','let','positive','apply','anger',\n",
    "'beg','bonus','comfortable','dislike','oppurtunity','obligation','improve','concerned','pick','open',\n",
    "'right away','job','rage','probably','refuse','upset','afraid','risk','alone','social anxiety','consequence',\n",
    "'agree','prove','fair','colleagues','awkward','paid','grow','avoid going','early','nervous','forc',\n",
    "'depend','resent','frustrated','difficult']\n",
    "\n",
    "e_low_3_emp_list=['social anxiety','extremely uncomfortable','nervous','social anxious','panic','unlikely',\n",
    "'impres','anxiety','probably','introvert','immediately','feel pressure','anxious','decline','emotion',\n",
    "'nerve-wracking','loner','pressure','avoid','stressed out','stressed','dislike','shy','hesitate','losing',\n",
    "'bad','difficult','not great at networking','obligation','unfair','stretch myself','hate networking',\n",
    "'willing','hell','stress','lose','nightmare','quit','avoid going','mad','paid','sad','reluctant','get out of it',\n",
    "'fair','not a social person','reluctance','quiet','upset','sacrifice','change','not comfortable','money',\n",
    "'tired','family','appeal','confirm','harm','prove','short ','skip','stick','hate','compensate','deserve',\n",
    "'short time','sick','outside of my comfort zone','deadline','pretend','discomfort','socially awkward',\n",
    "'show','angry','win','convince','not interested','apply','get along','negotiate','unpleasant','quickly',\n",
    "'awkward','not attend','concerned','plead','fire','suck it up','forc','comfortale','pick','uncomfortable',\n",
    "'unhappy','excuse','compromise','afraid','do not interact well with strangers',\"don't like being in social situation\",\n",
    "\"don't like networking\",\"don't like socializing\",'very shy']    \n",
    "\n",
    "e_high_3_emp_list=['career','good','frustrated','nice','best','deny','reflect','confident','grow','consequence',\n",
    "'missed out','connect','rage','importan','worry','I am sociable','party','right away','priority','sociable',\n",
    "'accept','focus','plan ','report','excited','reward','contribute','allow','success','contact','review',\n",
    "'absolutely go','for sure go','meet','great','colleagues','social','not need anyone','regardless','fool',\n",
    "'surely attend','leader','network','I like parties','no problem','learn','friendship','definitely go','introduce',\n",
    "'let','competition','client','make new friends']\n",
    "\n",
    "c_high_2_emp_list=['family','report','stress','question','convince','job','deserve','longest','comfortable','win','great time',\n",
    "'negative','fair','check in','short time','accus','short ','respect','willing','lie','correct','as soon',\n",
    "'positive','impres','review','problems','immediately','hate networking','anger','proof','upset','prove',\n",
    "'open','explain','improve','time ','confident','right away','let']    \n",
    "\n",
    "c_low_comb_emp_1_list=['hard stance','hardship','my right','call in','stare','contact','shy','quit','entitle','sad','reluctant','refuse',\n",
    "'demand','fool','wrong','get even','profanity','compensate','pressure','convince','responsible','fair','prove',\n",
    "'client','skip','get along','negotiate','sick','family','hell','difficult','sacrifice','obligation','awkward',\n",
    "'offer','friendship','hard ','career','threaten','party','allow','avoid','stick','hate','improve','terrible',\n",
    "'deadline','plan ','great','personal','plead','trouble','marked','respect','probably','early','confront',\n",
    "'lose','suck it up','better','excuse','suffer','busy','money','stress','beg','unfair','time ','personal issue',\n",
    "'deny','stressed']\n",
    "    \n",
    "c_low_comb_emp_3_list=['obligation','unlikely','panic','would not go','discomfort','deserve','awkward','immediately','unhappy',\n",
    "'social anxiety','skip','forc','social anxious','harm','probably','unpleasant','worthwhile','oppurtunity',\n",
    "'fire','negative','get along','shy','team','short ','open','pick','feel pressure','show','quit','angry',\n",
    "'avoid','hell','confirm','decline','money','socially awkward','negotiate','extremely uncomfortable','time ',\n",
    "'short time','not go','better','avoid going','not a social person','loner','valid','anger','hesitate',\n",
    "'hate networking','great time','positive','sociable','prove','emotion','compromise','allow','rage','concerned',\n",
    "'anxiety','outside of my comfort zone','wrong','deadline','stress','resent','personal issue','reconsider',\n",
    "'cold','not interested','unfair','early','drink','stressed']\n",
    "\n",
    "c_low_comb_emp_4_list=['probably','career','appeal','anxiety','depend','wrong','sad','cold','fool','marked','reflect','not go',\n",
    "'harm','bad','hate networking','job','mad','money','would change','reward','quit','focus','organize',\n",
    "'stressed','hesitate','leader','valid','difficult','consequence','emotion','personal issue','bonus',\n",
    "'accept','review','nice','avoid going','plan ','great time','lose','fire','frustrated','pressure','confront']\n",
    "    \n",
    "c_low_comb_emp_5_list=['forc','network','resent','considerate','difficult','I had to','frustrated','paid','obligation','nervous',\n",
    "'refuse','explain','rage','anger','grow','meet','respect','oppurtunity','bonus','nice','insist','job',\n",
    "'depend','avoid going','upset','awkward','positive','apply','short ','connect','probably','plan ','win',\n",
    "'open','concerned','question','negative','change','friendship','dislike','focus','alone']\n",
    "    \n",
    "c_high_comb_emp_1_list=['question','stubborn','would change','reconsider','as soon','human resources','disagree','defer','risk',\n",
    "'unpleasant','immediately','worry','argue','petty','explain','mad','proof','hurt','correct','obligated',\n",
    "'not go','harm','unhappy','leader','misunderstand','win','fire','unlikely','first','pick','angry','priority',\n",
    "'bonus','quickly','short time','hesitate','tense','social','switch','success','problems','not interested',\n",
    "'easy going','no problem','report','reflect','upset','anger','team','valid','paid','review','agree',\n",
    "'short ','willing','fun','concede','show','seniority','flexible','change']\n",
    "    \n",
    "c_high_comb_emp_3_list=['no problem','introvert','frustrated','win','sad','missed out','alone','dislike','appeal','depend','insist',\n",
    "'sick','success','not comfortable','stretch myself','report','benefit','accept','hard ','contribute','responsible',\n",
    "'compensate','fool','social','absolutely go','regardless','anyone','focus','pretend','worried','nightmare',\n",
    "'not need anyone','surely attend','for sure go','colleagues','competition','let','help','best','great',\n",
    "'deny','importan','learn','network','client','uncomfortable','priority','lie','improve','good','not attend',\n",
    "'fun','definitely go','mad','comfortable','reluctant','excited','excuse','meet']\n",
    "    \n",
    "c_high_comb_emp_4_list=['willing','change','respect','connect','fun','paid','hard ','immediately','terrible','grow','incorrect',\n",
    "'refuse','open','resent','quickly','contact','calm','party','short ','contribute','my right','stubborn',\n",
    "'rebut','problems','worried','as soon','compromise','hurt','good','proof','not true','early','human resources',\n",
    "'obligation','colleagues','meet','demand','success','negative','allow','concerned','disagree','let','agree']    \n",
    "    \n",
    "\n",
    "c_high_comb_emp_5_list=['success','appeal','worry','fun','busy','hesitate','problems','allow','hurt','improve','excited','good',\n",
    "'bad','leader','stress','importan','excuse','introduce','lose','enjoy','prove','personal issue','fair',\n",
    "'quickly','correct','stick','accus','unlikely','comfortable','sad','willing','contact','confus','career',\n",
    "'show','losing','immediately','compensate','anyone','lie','client','help','learn']\n",
    "    \n",
    "a_low_emp_1_list=['stare','responsible','fool','get even','profanity','call in','sick','refuse','emotion','hard stance','racist',\n",
    "'slash','hardship','demand','compensate','first','stick','quit','personal issue','excuse','trouble','deny',\n",
    "'hell','depend','money','cold','hard ','marked','pissed','client','deserve','unfair','fair',\n",
    "'resent','reconsider','offer','my right','hate','forc','worry','reward','reluctant','concerned','organize',\n",
    "'sad','losing','rage','bad','insist','busy','difficult','appeal','stressed out','stressed','wrong',\n",
    "'early','longest','proof','better','petty','improve','contact','avoid','accept','entitle','meet',\n",
    "'if I had to','seniority','suffer','comfortable','regardless','personal','not back down','stand my ground']  \n",
    "    \n",
    "a_high_emp_1_list=['agree','benefit','best','bonus','change','compromise','considerate','correct','defer','easy going','family',\n",
    "'flexible','fun','good','help','hurt','incorrect','leader','let','misunderstand','no problem','not interested',\n",
    "'obligation','paid','pick','priority','problems','quickly','respect','review','show','willing','win',\n",
    "'more than willing',\"don't want conflict\",'easy going','hate conflict','team player','avoid confrontation',\n",
    "'keep people happy']  \n",
    "    \n",
    "a_low_comb_emp_2_list=['wrong','question','busy','probably','resent','not go','importan','fun','enjoy','bad','first','problems',\n",
    "'refuse','better','short time','good','anxiety','avoid going','respect','compromise','losing','angry',\n",
    "'regardless','social anxiety','rage','decline','pretend','focus','connect','no problem','priority','excuse',\n",
    "'procrastinate','fool','sick','personal issue','anticipat','deadline','anyone','lose','difficult','meet',\n",
    "'judg','worry','plan ','trouble','show','nervous','reflect','help','pressure','compensate','bonus','get along',\n",
    "'flexible','colleagues','accus','fire','consequence','demand']\n",
    "    \n",
    "a_low_comb_emp_3_list=['I like parties','fire','unpleasant','would not go','sales','quit','discomfort','money','hate networking',\n",
    "'worthwhile','obligation','panic','emotion','unlikely','hell','skip','social anxious','pick','cold',\n",
    "'decline','not go','paid','get out of it','hate','reward','rage','short ','negotiate','beg','difficult',\n",
    "'trouble','resent','time ','immediately','stress','stressed','stressed out','reconsider','short time',\n",
    "'grow','extremely uncomfortable','willing','get along','apply','I had to','risk','anxiety','great',\n",
    "'forc','allow','socially awkward','dislike','I am sociable','great time','missed out','compensate','oppurtunity',\n",
    "'anger','benefit','plan ','confirm','avoid','social anxiety','fair','pressure','mad','deserve',\n",
    "'not a social person']\n",
    "    \n",
    "a_low_comb_emp_4_list=['rage','cold','fool','marked','depend','demand','quit','report','probably','career','accept','not go',\n",
    "'compensate','pressure','quiet','angry','afraid','confront','emotion','job','benefit','mad','threaten',\n",
    "'money','unpleasant','anxiety','pissed','anyone','obligation','confident','short ','regardless','refuse','appeal',\n",
    "'hesitate','examples','immediately','bad','suck it up','resent','respect','wrong','harm']\n",
    "    \n",
    "a_low_comb_emp_5_list=['paid','refuse','avoid going','alone','emotion','pretend','resent','bonus','win','rage','difficult',\n",
    "'probably','afraid','anger','forc','hate networking','change','agree','depend','pick','focus','obligation',\n",
    "'frustrated','considerate','right away','time ','money','negative','colleagues','awkward','improve','success',\n",
    "'explain','bad','best','respect','let','better','nice','nervous']\n",
    "    \n",
    "a_high_comb_emp_2_list=['agree','negative','benefit','overwhelmed','quiet','I had to','lie','team','check in','early','stick',\n",
    "'feel pressure','allow','family','sacrifice','stressed','learn','frustrated','right away','convince',\n",
    "'best','let','fair','client','longest','responsible','mad','stressed out','report','time ','upset',\n",
    "'confident','dislike','unhappy','anger','explain','positive','stress','proof']\n",
    "    \n",
    "a_high_comb_emp_3_list=['change','reluctant','angry','quickly','right away','excuse','stick','would change','early','compromise',\n",
    "'not comfortable','learn','positive','avoid going','anxious','colleagues','fool','reluctance','absolutely go',\n",
    "'fun','not attend','tired','losing','worry','busy','no problem','contribute','explain','hurt','network',\n",
    "'uncomfortable','consequence','social','not need anyone','surely attend','regardless','help','better',\n",
    "'excited','importan','priority','responsible','outside of my comfort zone','party','stretch myself','for sure go',\n",
    "'hard ','report','focus','client','alone','lie','introduce','friendship','comfortable','contact',\n",
    "'best','good','definitely go','anyone','meet']  \n",
    "    \n",
    "a_high_comb_emp_4_list=['convince','defend','lose','accus','worthwhile','agitated','personal','consequence','concerned','impres',\n",
    "'anger','success','correct','win','confus','argue','proof','incorrect','focus','terrible','best',\n",
    "'negative','not justified','as soon','plead','confirm','lie','unfair','early','judg','stressed out','hard ',\n",
    "'organize','risk','improve','worried','quickly','my right','open','frustrated','contact','meet',\n",
    "'compromise','pretend','rebut','stress','reconsider','hurt','would not go','importan','positive','problems',\n",
    "'agree','let','negotiate','allow','explain','learn','prove','better','anxious','colleagues','not true',\n",
    "'upset','grow']\n",
    "       \n",
    "a_high_comb_emp_5_list=['accept','short time','question','happy to go','excited','hard ','impres','good','grow','losing',\n",
    "'reward','show','contribute','convince','accus','willing','concerned','dislike','contact','hesitate',\n",
    "'network','comfortable','apply','leader','immediately','stress','correct','importan','great time','hurt',\n",
    "'offer','confus','help','anyone','lie','client','enjoy','learn']\n",
    "\n",
    "go_v2_3_list=['go for it','make an appearance','certainly go','would attend','just go','attend','still attend','still go',\n",
    "'would still go','definitely go','would go','all in','definitely be in attendance','absolutely go',\n",
    "'attend that meeting','cheerfully go','decide to go','definitely attend','definitely still go','go for sure',\n",
    "'go to the event','go to the meeting','go to the networking meeting','make sure I go','make time to attend',\n",
    "'still opt in','time and go']\n",
    "    \n",
    "not_go_v2_3_list=['would not go',\"wouldn't go\",'probably not go',\"wouldn't want to go\",'unlikely to go','decline',\n",
    "'not show up','hesitate to go','go home','ditch','avoid','get out of it','probably still go',\n",
    "'probably go','skip','decide to go','try to go','not going','in attendance','not interested',\n",
    "'come','not attend','would be going','not come','likely go','happy to go','probably attend',\n",
    "'bow out of the meeting','choose not to go','consider not going','hate going','likely not go',\n",
    "'not consider going','not feel like going','not want to go','politely decline','probably would not',\n",
    "\"probably wouldn't\",'stay at home','will not go']\n",
    "    \n",
    "not_go_5_list=['not go','not to go',\"n't go\"]\n",
    "\n",
    "go_5_list=['would go','probably go']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word List Section\n",
    "\n",
    "This section includes the code that was used in the word list prediction. The optimal weights were derived from feedback on the public leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute new word count variables from word lists\n",
    "\n",
    "write_keyword_count_column(df_total, 'not_go_5', 'open_ended_5', not_go_5_list)\n",
    "write_keyword_count_column(df_total, 'go_5', 'open_ended_5', go_5_list)\n",
    "\n",
    "df_total['go_comb_5']=df_total['go_5']-df_total['not_go_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'not_1', 'open_ended_1', not_list)\n",
    "write_keyword_count_column(df_total, 'not_2', 'open_ended_2', not_list)\n",
    "write_keyword_count_column(df_total, 'not_3', 'open_ended_3', not_list)\n",
    "write_keyword_count_column(df_total, 'not_4', 'open_ended_4', not_list)\n",
    "write_keyword_count_column(df_total, 'not_5', 'open_ended_5', not_list)\n",
    "\n",
    "df_total['sum_not']=df_total['not_1']+df_total['not_2']+df_total['not_3']+df_total['not_4']+df_total['not_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'no_5', 'open_ended_5', no_list)\n",
    "\n",
    "write_keyword_count_column(df_total, 'n_low_comb_emp_1', 'open_ended_1', n_low_comb_emp_1_list)\n",
    "write_keyword_count_column(df_total, 'n_low_comb_emp_2', 'open_ended_2', n_low_comb_emp_2_list)\n",
    "write_keyword_count_column(df_total, 'n_low_comb_emp_3', 'open_ended_3', n_low_comb_emp_3_list)\n",
    "write_keyword_count_column(df_total, 'n_low_comb_emp_5', 'open_ended_5', n_low_comb_emp_5_list)\n",
    "\n",
    "df_total['n_low_comb_emp']=df_total['n_low_comb_emp_1']+df_total['n_low_comb_emp_2']+df_total['n_low_comb_emp_3']+df_total['n_low_comb_emp_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'n_high_comb_emp_1', 'open_ended_1', n_high_comb_emp_1_list)\n",
    "write_keyword_count_column(df_total, 'n_high_comb_emp_2', 'open_ended_2', n_high_comb_emp_2_list)\n",
    "write_keyword_count_column(df_total, 'n_high_comb_emp_3', 'open_ended_3', n_high_comb_emp_3_list)\n",
    "write_keyword_count_column(df_total, 'n_high_comb_emp_5', 'open_ended_5', n_high_comb_emp_5_list)\n",
    "\n",
    "df_total['n_high_comb_emp']=df_total['n_high_comb_emp_1']+df_total['n_high_comb_emp_2']+df_total['n_high_comb_emp_3']+df_total['n_high_comb_emp_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'e_high_3', 'open_ended_3', e_high_3_list)\n",
    "write_keyword_count_column(df_total, 'e_high_4', 'open_ended_4', e_high_4_list)\n",
    "write_keyword_count_column(df_total, 'e_high_5', 'open_ended_5', e_high_5_list)\n",
    "\n",
    "df_total['e_high_3to5'] = df_total['e_high_3']+df_total['e_high_4']+df_total['e_high_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'go_5', 'open_ended_5', go_5_list)\n",
    "write_keyword_count_column(df_total, 'not_go_5', 'open_ended_5', not_go_5_list)\n",
    "\n",
    "df_total['go_comb_5']=df_total['go_5']-df_total['not_go_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'go_v2', 'open_ended_3', go_v2_3_list)\n",
    "write_keyword_count_column(df_total, 'not_go_v2', 'open_ended_3', not_go_v2_3_list)\n",
    "\n",
    "write_keyword_count_column(df_total, 'c_high_2_emp', 'open_ended_2', c_high_2_emp_list)\n",
    "\n",
    "write_keyword_count_column(df_total, 'c_low_comb_emp_1', 'open_ended_1', c_low_comb_emp_1_list)\n",
    "write_keyword_count_column(df_total, 'c_low_comb_emp_3', 'open_ended_2', c_low_comb_emp_3_list)\n",
    "write_keyword_count_column(df_total, 'c_low_comb_emp_4', 'open_ended_3', c_low_comb_emp_4_list)\n",
    "write_keyword_count_column(df_total, 'c_low_comb_emp_5', 'open_ended_5', c_low_comb_emp_5_list)\n",
    "\n",
    "df_total['c_low_comb_emp']=df_total['c_low_comb_emp_1']+df_total['c_low_comb_emp_3']+df_total['c_low_comb_emp_4']+df_total['c_low_comb_emp_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'c_high_comb_emp_1', 'open_ended_1', c_high_comb_emp_1_list)\n",
    "write_keyword_count_column(df_total, 'c_high_comb_emp_3', 'open_ended_2', c_high_comb_emp_3_list)\n",
    "write_keyword_count_column(df_total, 'c_high_comb_emp_4', 'open_ended_3', c_high_comb_emp_4_list)\n",
    "write_keyword_count_column(df_total, 'c_high_comb_emp_5', 'open_ended_5', c_high_comb_emp_5_list)\n",
    "\n",
    "df_total['c_high_comb_emp']=df_total['c_high_comb_emp_1']+df_total['c_high_comb_emp_3']+df_total['c_high_comb_emp_4']+df_total['c_high_comb_emp_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'e_low_3_emp', 'open_ended_2', e_low_3_emp_list)\n",
    "write_keyword_count_column(df_total, 'e_high_3_emp', 'open_ended_2', e_high_3_emp_list)\n",
    "\n",
    "write_keyword_count_column(df_total, 'a_low_1_emp', 'open_ended_1', a_low_emp_1_list)\n",
    "write_keyword_count_column(df_total, 'a_high_1_emp', 'open_ended_1', a_high_emp_1_list)\n",
    "\n",
    "write_keyword_count_column(df_total, 'a_low_comb_emp_2', 'open_ended_1', a_low_comb_emp_2_list)\n",
    "write_keyword_count_column(df_total, 'a_low_comb_emp_3', 'open_ended_2', a_low_comb_emp_3_list)\n",
    "write_keyword_count_column(df_total, 'a_low_comb_emp_4', 'open_ended_3', a_low_comb_emp_4_list)\n",
    "write_keyword_count_column(df_total, 'a_low_comb_emp_5', 'open_ended_5', a_low_comb_emp_5_list)\n",
    "\n",
    "df_total['a_low_comb_emp']=df_total['a_low_comb_emp_2']+df_total['a_low_comb_emp_2']+df_total['a_low_comb_emp_4']+df_total['a_low_comb_emp_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'a_high_comb_emp_2', 'open_ended_1', a_high_comb_emp_2_list)\n",
    "write_keyword_count_column(df_total, 'a_high_comb_emp_3', 'open_ended_2', a_high_comb_emp_3_list)\n",
    "write_keyword_count_column(df_total, 'a_high_comb_emp_4', 'open_ended_3', a_high_comb_emp_4_list)\n",
    "write_keyword_count_column(df_total, 'a_high_comb_emp_5', 'open_ended_5', a_high_comb_emp_5_list)\n",
    "\n",
    "df_total['a_high_comb_emp']=df_total['a_high_comb_emp_2']+df_total['a_high_comb_emp_2']+df_total['a_high_comb_emp_4']+df_total['a_high_comb_emp_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of features to standardize\n",
    "\n",
    "zvarlist=['char_count_3',\n",
    " 'char_count_4',\n",
    " 'avg_word_1',\n",
    " 'avg_word_2',\n",
    " 'avg_word_3',\n",
    " 'avg_word_4',\n",
    " 'avg_word_5',\n",
    " 'not_go_5',\n",
    " 'go_5',\n",
    " 'go_comb_5',\n",
    " 'sum_not',\n",
    " 'no_5',\n",
    " 'not_5',\n",
    " 'n_low_comb_emp',\n",
    " 'n_high_comb_emp',\n",
    " 'e_high_3',\n",
    " 'e_high_4',\n",
    " 'e_high_5',\n",
    " 'e_high_3to5',\n",
    " 'go_v2',\n",
    " 'not_go_v2',\n",
    " 'c_high_2_emp',\n",
    " 'c_low_comb_emp',\n",
    " 'c_high_comb_emp',\n",
    " 'e_low_3_emp',\n",
    " 'e_high_3_emp',\n",
    " 'a_low_1_emp',\n",
    " 'a_high_1_emp',\n",
    " 'a_low_comb_emp',\n",
    " 'a_high_comb_emp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize list\n",
    "\n",
    "cols = zvarlist\n",
    "for col in cols:\n",
    "    col_zscore = 'Z'+ col\n",
    "    df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighting features\n",
    "\n",
    "df_total['Zno_5']=df_total['Zno_5'] * -1\n",
    "df_total['Znot_5']=df_total['Znot_5']  *-1\n",
    "df_total['Zn_low_comb_emp']=df_total['Zn_low_comb_emp']  *-1\n",
    "df_total['Zc_high_2_emp']=df_total['Zc_high_2_emp']  *1.25\n",
    "df_total['Zc_low_comb_emp']=df_total['Zc_low_comb_emp']  *-1\n",
    "df_total['Zc_high_comb_emp']=df_total['Zc_high_comb_emp'] *1.5\n",
    "df_total['Ze_low_3_emp']=df_total['Ze_low_3_emp']  *-1.25\n",
    "df_total['Ze_high_3_emp']=df_total['Ze_high_3_emp']  *1.25\n",
    "df_total['Znot_go_v2']=df_total['Znot_go_v2']  *-1\n",
    "df_total['Zsum_not']=df_total['Zsum_not']  *-1\n",
    "df_total['Za_low_1_emp ']=df_total['Za_low_1_emp']  *-1.5\n",
    "df_total['Za_low_comb_emp']=df_total['Za_low_comb_emp']  *-1\n",
    "df_total['Za_high_comb_emp']=df_total['Za_high_comb_emp']  *1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['o_pred']=df_total[['Zchar_count_3', 'Zchar_count_4', 'Zno_5', 'Znot_5', 'Ze_high_3to5',  'Zavg_word_2', \n",
    "                         'Zavg_word_4', 'Zgo_comb_5']].mean(axis=1)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recode o_pred\n",
    "\n",
    "recode_list=df_total[['o_pred']]\n",
    "\n",
    "def recode_extreme(predictor_col):\n",
    "    if predictor_col >=0.75:\n",
    "        val=.75\n",
    "    else: \n",
    "        val=predictor_col\n",
    "    return val\n",
    "\n",
    "for predictor_col in recode_list:\n",
    "    df_total[predictor_col] = df_total[predictor_col].apply(recode_extreme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['n_pred'] = df_total[['Zn_low_comb_emp', 'Zn_high_comb_emp']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['c_pred'] = df_total[['Zc_high_2_emp','Zc_low_comb_emp','Zc_high_comb_emp','Zavg_word_5']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['e_pred'] = df_total[['Ze_low_3_emp', 'Ze_high_3_emp', 'Zgo_v2', 'Znot_go_v2']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['a_pred'] = df_total[['Zavg_word_4', 'Zsum_not', 'Zavg_word_5', 'Za_low_1_emp', 'Za_high_1_emp', 'Za_low_comb_emp','Za_high_comb_emp']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Machine Learning Section\n",
    "\n",
    "Much of the machine learning that we applied did not result in stronger predictions on the public leaderboard compare to the word lists. Therefore, much of these exploratory features have been removed. We retained what was ultimately submitted to the private leaderboard.\n",
    "\n",
    "Not all the features that are defined here are important to the prediction. Again, we were lazy and did not prune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllables_count(text): \n",
    "    return textstatistics().syllable_count(text) \n",
    "\n",
    "def difficult_word_count(text):\n",
    "    return textstatistics().difficult_words(text)\n",
    "\n",
    "def sentence_count(text):\n",
    "    return textstatistics().sentence_count(text)\n",
    "\n",
    "def avg_syllables_per_word(text): \n",
    "    nsyllables=syllables_count(text)\n",
    "    nwords=word_count(text)\n",
    "    ASPW=float(nsyllables)/float(nwords)\n",
    "    return legacy_round(ASPW,2)\n",
    "\n",
    "def avg_sentence_length(text): \n",
    "    nwords = word_count(text) \n",
    "    nsentences = sentence_count(text) \n",
    "    average_sentence_length = float(nwords / nsentences) \n",
    "    return legacy_round(average_sentence_length,2)\n",
    "  \n",
    "def flesch_ease_score(text):\n",
    "    return textstatistics().flesch_reading_ease(text)\n",
    "    \n",
    "def flesch_grade_score(text):\n",
    "    return textstatistics().flesch_kincaid_grade(text)\n",
    "\n",
    "def linsear_write_score(text):\n",
    "    return textstatistics().linsear_write_formula(text)\n",
    "\n",
    "def dale_chall_score(text):\n",
    "    return textstatistics().dale_chall_readability_score(text)\n",
    "\n",
    "def gunning_fog_score(text):\n",
    "    return textstatistics().gunning_fog(text)\n",
    "\n",
    "def smog_score(text):\n",
    "    return textstatistics().smog_index(text)\n",
    "\n",
    "def automated_readability_score(text):\n",
    "    return textstatistics().automated_readability_index(text)\n",
    "\n",
    "def coleman_liau_score(text):\n",
    "    return textstatistics().coleman_liau_index(text)\n",
    "\n",
    "# This function is supposed to count grammatical errors. \n",
    "def lang_checker(text):\n",
    "    tool = language_check.LanguageTool('en-US')\n",
    "    count=0\n",
    "    matches = tool.check(text)\n",
    "    for i in range(len(matches)-1):\n",
    "        if matches[i].ruleId == 'WHITESPACE_RULE':\n",
    "            pass\n",
    "        else:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def tokenize(text):\n",
    "    return TextBlob(text).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute some spelling-based features\n",
    "for predictor_col in PREDICTOR_TEXT_COLUMN_NAMES_ALL:\n",
    "    df_total[predictor_col + \"_num_chars\"] = df_total[predictor_col].apply(len)\n",
    "    df_total[predictor_col + \"_num_words\"] = df_total[predictor_col].apply(word_count)\n",
    "    df_total[predictor_col + \"_num_misspelled\"] = df_total[predictor_col].apply(compute_num_spelling_errors)\n",
    "    df_total[predictor_col + \"_flesch_grade\"] = df_total[predictor_col].apply(flesch_grade_score) \n",
    "    df_total[predictor_col + \"_percent_misspelled\"] = df_total[[predictor_col + \"_num_misspelled\",\n",
    "                              predictor_col + \"_num_words\"\n",
    "    ]].apply(lambda x: divide(*x), axis=1)\n",
    "\n",
    "# Compute readability features\n",
    "df_total[\"readability_syllables_count\"] = df_total['open_ended_6'].apply(syllables_count) \n",
    "df_total[\"readability_word_count\"] = df_total['open_ended_6'].apply(word_count) \n",
    "df_total[\"readability_difficult_count\"] = df_total['open_ended_6'].apply(difficult_word_count) \n",
    "df_total[\"readability_sentence_count\"] = df_total['open_ended_6'].apply(sentence_count) \n",
    "df_total[\"readability_avg_syllables_per_word\"] = df_total['open_ended_6'].apply(avg_syllables_per_word)\n",
    "df_total[\"readability_avg_sentence_length\"] = df_total['open_ended_6'].apply(avg_sentence_length) \n",
    "df_total[\"readability_flesch_ease_score\"] = df_total['open_ended_6'].apply(flesch_ease_score) \n",
    "df_total[\"readability_flesch_grade_score\"] = df_total['open_ended_6'].apply(flesch_grade_score) \n",
    "df_total[\"readability_linsear_write_score\"] = df_total['open_ended_6'].apply(linsear_write_score) \n",
    "df_total[\"readability_dale_chall_score\"] = df_total['open_ended_6'].apply(dale_chall_score) \n",
    "df_total[\"readability_smog_score\"] = df_total['open_ended_6'].apply(smog_score) \n",
    "df_total[\"readability_coleman_liau_score\"] = df_total['open_ended_6'].apply(coleman_liau_score) \n",
    "\n",
    "# Compute variable for the number of grammar errors based on Nick's function. \n",
    "df_total[\"number_grammar_errors\"] = df_total['open_ended_6'].apply(lang_checker)\n",
    "  \n",
    "# Compute Average Word Length for each open ended comment.     \n",
    "df_total['Avg_word_length_1']=df_total['open_ended_1_num_chars']/df_total['open_ended_1_num_words']\n",
    "df_total['Avg_word_length_2']=df_total['open_ended_2_num_chars']/df_total['open_ended_2_num_words']\n",
    "df_total['Avg_word_length_3']=df_total['open_ended_3_num_chars']/df_total['open_ended_3_num_words']\n",
    "df_total['Avg_word_length_4']=df_total['open_ended_4_num_chars']/df_total['open_ended_4_num_words']\n",
    "df_total['Avg_word_length_5']=df_total['open_ended_5_num_chars']/df_total['open_ended_5_num_words']\n",
    "df_total['Avg_word_length_6']=df_total['open_ended_6_num_chars']/df_total['open_ended_6_num_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A_Scale_score',\n",
       " 'C_Scale_score',\n",
       " 'E_Scale_score',\n",
       " 'N_Scale_score',\n",
       " 'O_Scale_score',\n",
       " 'Respondent_ID',\n",
       " 'open_ended_1_num_words',\n",
       " 'open_ended_1_num_misspelled',\n",
       " 'open_ended_1_percent_misspelled',\n",
       " 'open_ended_2_num_words',\n",
       " 'open_ended_2_num_misspelled',\n",
       " 'open_ended_2_percent_misspelled',\n",
       " 'open_ended_3_num_words',\n",
       " 'open_ended_3_num_misspelled',\n",
       " 'open_ended_3_percent_misspelled',\n",
       " 'open_ended_4_num_words',\n",
       " 'open_ended_4_num_misspelled',\n",
       " 'open_ended_4_percent_misspelled',\n",
       " 'open_ended_5_num_words',\n",
       " 'open_ended_5_num_misspelled',\n",
       " 'open_ended_5_percent_misspelled',\n",
       " 'open_ended_6_num_words',\n",
       " 'open_ended_6_num_misspelled',\n",
       " 'open_ended_6_percent_misspelled',\n",
       " 'O_high_5',\n",
       " 'C_high_2',\n",
       " 'A_high_1',\n",
       " 'E_high_3',\n",
       " 'A_low_2',\n",
       " 'A_low_3',\n",
       " 'A_low_4',\n",
       " 'A_low_5',\n",
       " 'N_low_1',\n",
       " 'N_low_2',\n",
       " 'N_low_3',\n",
       " 'N_low_5',\n",
       " 'C_high_1',\n",
       " 'C_high_3',\n",
       " 'C_high_4',\n",
       " 'C_high_5',\n",
       " 'A_high_2',\n",
       " 'A_high_3',\n",
       " 'A_high_4',\n",
       " 'A_high_5',\n",
       " 'N_high_1',\n",
       " 'N_high_2',\n",
       " 'N_high_3',\n",
       " 'N_high_5',\n",
       " 'O_high_1',\n",
       " 'O_high_2',\n",
       " 'O_high_3',\n",
       " 'O_high_4',\n",
       " 'E_high_4',\n",
       " 'E_high_5',\n",
       " 'A_low_1',\n",
       " 'E_low_3',\n",
       " 'GO_3',\n",
       " 'NOGO_3',\n",
       " 'GO_5',\n",
       " 'NOGO_5',\n",
       " 'NOT_1',\n",
       " 'NOT_2',\n",
       " 'NOT_3',\n",
       " 'NOT_4',\n",
       " 'NOT_5',\n",
       " 'NO_5',\n",
       " 'A_low_comb',\n",
       " 'N_low_comb',\n",
       " 'C_high_comb',\n",
       " 'A_high_comb',\n",
       " 'N_high_comb',\n",
       " 'O_high_comb',\n",
       " 'E_high_3to5',\n",
       " 'A_not_comb',\n",
       " 'O_go_comb',\n",
       " 'char_count_3',\n",
       " 'char_count_4',\n",
       " 'avg_word_1',\n",
       " 'avg_word_2',\n",
       " 'avg_word_3',\n",
       " 'avg_word_4',\n",
       " 'avg_word_5',\n",
       " 'not_go_5',\n",
       " 'go_5',\n",
       " 'go_comb_5',\n",
       " 'not_1',\n",
       " 'not_2',\n",
       " 'not_3',\n",
       " 'not_4',\n",
       " 'not_5',\n",
       " 'sum_not',\n",
       " 'no_5',\n",
       " 'n_low_comb_emp_1',\n",
       " 'n_low_comb_emp_2',\n",
       " 'n_low_comb_emp_3',\n",
       " 'n_low_comb_emp_5',\n",
       " 'n_low_comb_emp',\n",
       " 'n_high_comb_emp_1',\n",
       " 'n_high_comb_emp_2',\n",
       " 'n_high_comb_emp_3',\n",
       " 'n_high_comb_emp_5',\n",
       " 'n_high_comb_emp',\n",
       " 'e_high_3',\n",
       " 'e_high_4',\n",
       " 'e_high_5',\n",
       " 'e_high_3to5',\n",
       " 'go_v2',\n",
       " 'not_go_v2',\n",
       " 'c_high_2_emp',\n",
       " 'c_low_comb_emp_1',\n",
       " 'c_low_comb_emp_3',\n",
       " 'c_low_comb_emp_4',\n",
       " 'c_low_comb_emp_5',\n",
       " 'c_low_comb_emp',\n",
       " 'c_high_comb_emp_1',\n",
       " 'c_high_comb_emp_3',\n",
       " 'c_high_comb_emp_4',\n",
       " 'c_high_comb_emp_5',\n",
       " 'c_high_comb_emp',\n",
       " 'e_low_3_emp',\n",
       " 'e_high_3_emp',\n",
       " 'a_low_1_emp',\n",
       " 'a_high_1_emp',\n",
       " 'a_low_comb_emp_2',\n",
       " 'a_low_comb_emp_3',\n",
       " 'a_low_comb_emp_4',\n",
       " 'a_low_comb_emp_5',\n",
       " 'a_low_comb_emp',\n",
       " 'a_high_comb_emp_2',\n",
       " 'a_high_comb_emp_3',\n",
       " 'a_high_comb_emp_4',\n",
       " 'a_high_comb_emp_5',\n",
       " 'a_high_comb_emp',\n",
       " 'Zchar_count_3',\n",
       " 'Zchar_count_4',\n",
       " 'Zavg_word_1',\n",
       " 'Zavg_word_2',\n",
       " 'Zavg_word_3',\n",
       " 'Zavg_word_4',\n",
       " 'Zavg_word_5',\n",
       " 'Znot_go_5',\n",
       " 'Zgo_5',\n",
       " 'Zgo_comb_5',\n",
       " 'Zsum_not',\n",
       " 'Zno_5',\n",
       " 'Znot_5',\n",
       " 'Zn_low_comb_emp',\n",
       " 'Zn_high_comb_emp',\n",
       " 'Ze_high_3',\n",
       " 'Ze_high_4',\n",
       " 'Ze_high_5',\n",
       " 'Ze_high_3to5',\n",
       " 'Zgo_v2',\n",
       " 'Znot_go_v2',\n",
       " 'Zc_high_2_emp',\n",
       " 'Zc_low_comb_emp',\n",
       " 'Zc_high_comb_emp',\n",
       " 'Ze_low_3_emp',\n",
       " 'Ze_high_3_emp',\n",
       " 'Za_low_1_emp',\n",
       " 'Za_high_1_emp',\n",
       " 'Za_low_comb_emp',\n",
       " 'Za_high_comb_emp',\n",
       " 'Za_low_1_emp ',\n",
       " 'o_pred',\n",
       " 'n_pred',\n",
       " 'c_pred',\n",
       " 'e_pred',\n",
       " 'a_pred',\n",
       " 'open_ended_1_num_chars',\n",
       " 'open_ended_1_flesch_grade',\n",
       " 'open_ended_2_num_chars',\n",
       " 'open_ended_2_flesch_grade',\n",
       " 'open_ended_3_num_chars',\n",
       " 'open_ended_3_flesch_grade',\n",
       " 'open_ended_4_num_chars',\n",
       " 'open_ended_4_flesch_grade',\n",
       " 'open_ended_5_num_chars',\n",
       " 'open_ended_5_flesch_grade',\n",
       " 'open_ended_6_num_chars',\n",
       " 'open_ended_6_flesch_grade',\n",
       " 'readability_syllables_count',\n",
       " 'readability_word_count',\n",
       " 'readability_difficult_count',\n",
       " 'readability_sentence_count',\n",
       " 'readability_avg_syllables_per_word',\n",
       " 'readability_avg_sentence_length',\n",
       " 'readability_flesch_ease_score',\n",
       " 'readability_flesch_grade_score',\n",
       " 'readability_linsear_write_score',\n",
       " 'readability_dale_chall_score',\n",
       " 'readability_smog_score',\n",
       " 'readability_coleman_liau_score',\n",
       " 'number_grammar_errors',\n",
       " 'Avg_word_length_1',\n",
       " 'Avg_word_length_2',\n",
       " 'Avg_word_length_3',\n",
       " 'Avg_word_length_4',\n",
       " 'Avg_word_length_5',\n",
       " 'Avg_word_length_6']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of the numeric columns to paste into the Z-Score variable list\n",
    "FEATURES = df_total.select_dtypes(include=[np.number]).columns.tolist()\n",
    "FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Z Scores for all new feastures\n",
    "\n",
    "cols = FEATURES\n",
    "for col in cols:\n",
    "    col_zscore = 'Z_'+ col\n",
    "    df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_Var_List=[\n",
    " 'Z_open_ended_1_num_words',\n",
    " 'Z_open_ended_1_num_misspelled',\n",
    " 'Z_open_ended_1_percent_misspelled',\n",
    " 'Z_open_ended_2_num_words',\n",
    " 'Z_open_ended_2_num_misspelled',\n",
    " 'Z_open_ended_2_percent_misspelled',\n",
    " 'Z_open_ended_3_num_words',\n",
    " 'Z_open_ended_3_num_misspelled',\n",
    " 'Z_open_ended_3_percent_misspelled',\n",
    " 'Z_open_ended_4_num_words',\n",
    " 'Z_open_ended_4_num_misspelled',\n",
    " 'Z_open_ended_4_percent_misspelled',\n",
    " 'Z_open_ended_5_num_words',\n",
    " 'Z_open_ended_5_num_misspelled',\n",
    " 'Z_open_ended_5_percent_misspelled',\n",
    " 'Z_open_ended_6_num_words',\n",
    " 'Z_open_ended_6_num_misspelled',\n",
    " 'Z_open_ended_6_percent_misspelled',\n",
    " 'Z_O_high_5',\n",
    " 'Z_C_high_2',\n",
    " 'Z_A_high_1',\n",
    " 'Z_E_high_3',\n",
    " 'Z_A_low_2',\n",
    " 'Z_A_low_3',\n",
    " 'Z_A_low_4',\n",
    " 'Z_A_low_5',\n",
    " 'Z_N_low_1',\n",
    " 'Z_N_low_2',\n",
    " 'Z_N_low_3',\n",
    " 'Z_N_low_5',\n",
    " 'Z_C_high_1',\n",
    " 'Z_C_high_3',\n",
    " 'Z_C_high_4',\n",
    " 'Z_C_high_5',\n",
    " 'Z_A_high_2',\n",
    " 'Z_A_high_3',\n",
    " 'Z_A_high_4',\n",
    " 'Z_A_high_5',\n",
    " 'Z_N_high_1',\n",
    " 'Z_N_high_2',\n",
    " 'Z_N_high_3',\n",
    " 'Z_N_high_5',\n",
    " 'Z_O_high_1',\n",
    " 'Z_O_high_2',\n",
    " 'Z_O_high_3',\n",
    " 'Z_O_high_4',\n",
    " 'Z_E_high_4',\n",
    " 'Z_E_high_5',\n",
    " 'Z_A_low_1',\n",
    " 'Z_E_low_3',\n",
    " 'Z_GO_3',\n",
    " 'Z_NOGO_3',\n",
    " 'Z_GO_5',\n",
    " 'Z_NOGO_5',\n",
    " 'Z_NOT_1',\n",
    " 'Z_NOT_2',\n",
    " 'Z_NOT_3',\n",
    " 'Z_NOT_4',\n",
    " 'Z_NOT_5',\n",
    " 'Z_NO_5',\n",
    " 'Z_A_low_comb',\n",
    " 'Z_N_low_comb',\n",
    " 'Z_C_high_comb',\n",
    " 'Z_A_high_comb',\n",
    " 'Z_N_high_comb',\n",
    " 'Z_O_high_comb',\n",
    " 'Z_E_high_3to5',\n",
    " 'Z_A_not_comb',\n",
    " 'Z_O_go_comb',\n",
    " 'Z_open_ended_1_num_chars',\n",
    " 'Z_open_ended_1_flesch_grade',\n",
    " 'Z_open_ended_2_num_chars',\n",
    " 'Z_open_ended_2_flesch_grade',\n",
    " 'Z_open_ended_3_num_chars',\n",
    " 'Z_open_ended_3_flesch_grade',\n",
    " 'Z_open_ended_4_num_chars',\n",
    " 'Z_open_ended_4_flesch_grade',\n",
    " 'Z_open_ended_5_num_chars',\n",
    " 'Z_open_ended_5_flesch_grade',\n",
    " 'Z_open_ended_6_num_chars',\n",
    " 'Z_open_ended_6_flesch_grade',\n",
    " 'Z_readability_syllables_count',\n",
    " 'Z_readability_word_count',\n",
    " 'Z_readability_difficult_count',\n",
    " 'Z_readability_sentence_count',\n",
    " 'Z_readability_avg_syllables_per_word',\n",
    " 'Z_readability_avg_sentence_length',\n",
    " 'Z_readability_flesch_ease_score',\n",
    " 'Z_readability_flesch_grade_score',\n",
    " 'Z_readability_linsear_write_score',\n",
    " 'Z_readability_dale_chall_score',\n",
    " 'Z_readability_smog_score',\n",
    " 'Z_readability_coleman_liau_score',\n",
    " 'Z_number_grammar_errors',\n",
    " 'Z_Avg_word_length_1',\n",
    " 'Z_Avg_word_length_2',\n",
    " 'Z_Avg_word_length_3',\n",
    " 'Z_Avg_word_length_4',\n",
    " 'Z_Avg_word_length_5',\n",
    " 'Z_Avg_word_length_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Z_open_ended_1_num_words',\n",
       " 'Z_open_ended_1_num_misspelled',\n",
       " 'Z_open_ended_1_percent_misspelled',\n",
       " 'Z_open_ended_2_num_words',\n",
       " 'Z_open_ended_2_num_misspelled',\n",
       " 'Z_open_ended_2_percent_misspelled',\n",
       " 'Z_open_ended_3_num_words',\n",
       " 'Z_open_ended_3_num_misspelled',\n",
       " 'Z_open_ended_3_percent_misspelled',\n",
       " 'Z_open_ended_4_num_words',\n",
       " 'Z_open_ended_4_num_misspelled',\n",
       " 'Z_open_ended_4_percent_misspelled',\n",
       " 'Z_open_ended_5_num_words',\n",
       " 'Z_open_ended_5_num_misspelled',\n",
       " 'Z_open_ended_5_percent_misspelled',\n",
       " 'Z_open_ended_6_num_words',\n",
       " 'Z_open_ended_6_num_misspelled',\n",
       " 'Z_open_ended_6_percent_misspelled',\n",
       " 'Z_O_high_5',\n",
       " 'Z_C_high_2',\n",
       " 'Z_A_high_1',\n",
       " 'Z_E_high_3',\n",
       " 'Z_A_low_2',\n",
       " 'Z_A_low_3',\n",
       " 'Z_A_low_4',\n",
       " 'Z_A_low_5',\n",
       " 'Z_N_low_1',\n",
       " 'Z_N_low_2',\n",
       " 'Z_N_low_3',\n",
       " 'Z_N_low_5',\n",
       " 'Z_C_high_1',\n",
       " 'Z_C_high_3',\n",
       " 'Z_C_high_4',\n",
       " 'Z_C_high_5',\n",
       " 'Z_A_high_2',\n",
       " 'Z_A_high_3',\n",
       " 'Z_A_high_4',\n",
       " 'Z_A_high_5',\n",
       " 'Z_N_high_1',\n",
       " 'Z_N_high_2',\n",
       " 'Z_N_high_3',\n",
       " 'Z_N_high_5',\n",
       " 'Z_O_high_1',\n",
       " 'Z_O_high_2',\n",
       " 'Z_O_high_3',\n",
       " 'Z_O_high_4',\n",
       " 'Z_E_high_4',\n",
       " 'Z_E_high_5',\n",
       " 'Z_A_low_1',\n",
       " 'Z_E_low_3',\n",
       " 'Z_GO_3',\n",
       " 'Z_NOGO_3',\n",
       " 'Z_GO_5',\n",
       " 'Z_NOGO_5',\n",
       " 'Z_NOT_1',\n",
       " 'Z_NOT_2',\n",
       " 'Z_NOT_3',\n",
       " 'Z_NOT_4',\n",
       " 'Z_NOT_5',\n",
       " 'Z_NO_5',\n",
       " 'Z_A_low_comb',\n",
       " 'Z_N_low_comb',\n",
       " 'Z_C_high_comb',\n",
       " 'Z_A_high_comb',\n",
       " 'Z_N_high_comb',\n",
       " 'Z_O_high_comb',\n",
       " 'Z_E_high_3to5',\n",
       " 'Z_A_not_comb',\n",
       " 'Z_O_go_comb',\n",
       " 'Z_open_ended_1_num_chars',\n",
       " 'Z_open_ended_1_flesch_grade',\n",
       " 'Z_open_ended_2_num_chars',\n",
       " 'Z_open_ended_2_flesch_grade',\n",
       " 'Z_open_ended_3_num_chars',\n",
       " 'Z_open_ended_3_flesch_grade',\n",
       " 'Z_open_ended_4_num_chars',\n",
       " 'Z_open_ended_4_flesch_grade',\n",
       " 'Z_open_ended_5_num_chars',\n",
       " 'Z_open_ended_5_flesch_grade',\n",
       " 'Z_open_ended_6_num_chars',\n",
       " 'Z_open_ended_6_flesch_grade',\n",
       " 'Z_readability_syllables_count',\n",
       " 'Z_readability_word_count',\n",
       " 'Z_readability_difficult_count',\n",
       " 'Z_readability_sentence_count',\n",
       " 'Z_readability_avg_syllables_per_word',\n",
       " 'Z_readability_avg_sentence_length',\n",
       " 'Z_readability_flesch_ease_score',\n",
       " 'Z_readability_flesch_grade_score',\n",
       " 'Z_readability_linsear_write_score',\n",
       " 'Z_readability_dale_chall_score',\n",
       " 'Z_readability_smog_score',\n",
       " 'Z_readability_coleman_liau_score',\n",
       " 'Z_number_grammar_errors',\n",
       " 'Z_Avg_word_length_1',\n",
       " 'Z_Avg_word_length_2',\n",
       " 'Z_Avg_word_length_3',\n",
       " 'Z_Avg_word_length_4',\n",
       " 'Z_Avg_word_length_5',\n",
       " 'Z_Avg_word_length_6']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_Var_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset dataframes. \n",
    "\n",
    "df_train=df_total.loc[df_total['Source']=='Train'] \n",
    "df_test=df_total.loc[df_total['Source']=='Test'] \n",
    "df_final=df_total.loc[df_total['Source']=='Final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f033be72524f51ae41afcaed7a3ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=420, style=ProgressStyle(descript"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = df_train[Z_Var_List]\n",
    "Y = df_train['O_Scale_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.60, test_size=0.40)\n",
    "O_pipeline_optimizer = TPOTRegressor(generations=20, population_size=20, cv=5,random_state=42, verbosity=2)\n",
    "O_pipeline_optimizer.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[Z_Var_List]\n",
    "Y = df_train['C_Scale_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.60, test_size=0.40)\n",
    "C_pipeline_optimizer = TPOTRegressor(generations=20, population_size=20, cv=5,random_state=42, verbosity=2)\n",
    "C_pipeline_optimizer.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[Z_Var_List]\n",
    "Y = df_train['E_Scale_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.60, test_size=0.40)\n",
    "E_pipeline_optimizer = TPOTRegressor(generations=20, population_size=20, cv=5,random_state=42, verbosity=2)\n",
    "E_pipeline_optimizer.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[Z_Var_List]\n",
    "Y = df_train['A_Scale_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.60, test_size=0.40)\n",
    "A_pipeline_optimizer = TPOTRegressor(generations=20, population_size=20, cv=5,random_state=42, verbosity=2)\n",
    "A_pipeline_optimizer.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9207845297cc4bed8d5860e59abe9307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=420, style=ProgressStyle(descript"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: -0.26063485011127413\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: ElasticNetCV(input_matrix, l1_ratio=0.65, tol=0.01)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTRegressor(config_dict=None, crossover_rate=0.1, cv=5,\n",
       "       disable_update_check=False, early_stop=None, generations=20,\n",
       "       max_eval_time_mins=5, max_time_mins=None, memory=None,\n",
       "       mutation_rate=0.9, n_jobs=1, offspring_size=None,\n",
       "       periodic_checkpoint_folder=None, population_size=20,\n",
       "       random_state=42, scoring=None, subsample=1.0, use_dask=False,\n",
       "       verbosity=2, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Auto ML with feature set predicting Agreeableness\n",
    "X = df_train[Z_Var_List]\n",
    "Y = df_train['N_Scale_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.60, test_size=0.40)\n",
    "N_pipeline_optimizer = TPOTRegressor(generations=20, population_size=20, cv=5,random_state=42, verbosity=2)\n",
    "N_pipeline_optimizer.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m0a00q3\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\m0a00q3\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Save predicted values\n",
    "\n",
    "df_test['O_Scale_pred'] = O_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
    "df_test['C_Scale_pred'] = C_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
    "df_test['E_Scale_pred'] = E_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
    "df_test['A_Scale_pred'] = A_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
    "df_test['N_Scale_pred'] = N_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
    "\n",
    "df_final['O_Scale_pred'] = O_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
    "df_final['C_Scale_pred'] = C_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
    "df_final['E_Scale_pred'] = E_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
    "df_final['A_Scale_pred'] = A_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
    "df_final['N_Scale_pred'] = N_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
    "\n",
    "df_test.to_csv(\"ML Test.csv\", index = False)\n",
    "df_final.to_csv(\"ML Final.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Section\n",
    "\n",
    "We wrote this section to run independently from the previous sections (i.e., there are no independencies). In this way, you can see how we used deep learning without getting confused with all the other junk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten, Input, MaxPooling1D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import backend as K \n",
    "import keras.layers as layers\n",
    "from keras.models import Model, load_model\n",
    "from keras.engine import Layer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Initialize session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have a GPU - else this'll take a lifetime or two\n",
    "sess.list_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure directory hierarchy aligns\n",
    "train_raw_df = pd.read_csv(\"../materials/Data/siop_ml_train_participant.csv\")\n",
    "df_test = pd.read_csv(\"../materials/Data/siop_ml_test_participant.csv\")\n",
    "df_dev = pd.read_csv(\"../materials/Data/siop_ml_dev_participant.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTRIBUTE_LIST = [\"E\", \"A\", \"O\", \"C\", \"N\"]\n",
    "\n",
    "X = train_raw_df[['open_ended_' + str(idx) for idx in range(1, 6)]]\n",
    "Y = np.array(train_raw_df[[att + \"_Scale_score\" for att in ATTRIBUTE_LIST]].values)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    test_size=0.2,\n",
    "    random_state=23\n",
    ")\n",
    "\n",
    "X_train = [X_train['open_ended_' + str(idx)] for idx in range(1, 6)]\n",
    "X_test = [X_test['open_ended_' + str(idx)] for idx in range(1, 6)]\n",
    "X_dev = [df_test['open_ended_' + str(idx)] for idx in range(1, 6)]\n",
    "X_dev_ = [df_dev['open_ended_' + str(idx)] for idx in range(1, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom layer that allows us to update weights (lambda layers do not have trainable parameters!)\n",
    "\n",
    "class ElmoEmbeddingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.dimensions = 1024\n",
    "        self.trainable=True\n",
    "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,\n",
    "                               name=\"{}_module\".format(self.name))\n",
    "\n",
    "        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
    "        super(ElmoEmbeddingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n",
    "                      as_dict=True,\n",
    "                      signature='default',\n",
    "                      )['default']\n",
    "        return result\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return K.not_equal(inputs, '--PAD--')\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ElmoRegressionModel(\n",
    "    dense_dropout_rate=0.5,\n",
    "    loss='mean_squared_error',\n",
    "    optimizer='adam',\n",
    "    metrics=['mse'],\n",
    "    print_summary=False,\n",
    "    include_hidden_layer=False,\n",
    "    hidden_layer_size=64\n",
    "):\n",
    "    inputs, embeddings = [], []\n",
    "    \n",
    "    for idx in range(1, 6):\n",
    "        _input = layers.Input(shape=(1,), dtype=\"string\")\n",
    "        inputs.append(_input)\n",
    "        embedding = ElmoEmbeddingLayer()(_input)\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "    concat = layers.concatenate(embeddings)\n",
    "    dense = Dropout(dense_dropout_rate)(concat)\n",
    "    if include_hidden_layer:\n",
    "        dense = layers.Dense(hidden_layer_size, activation='relu')(dense)\n",
    "        dense = Dropout(dense_dropout_rate)(dense)\n",
    "    dense = layers.Dense(1, activation='relu')(dense)# (drop2)\n",
    "    \n",
    "    # If we want to do 5-way prediction within a single network\n",
    "    # dense = layers.Dense(5, activation='relu')(dense)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=dense)\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    if print_summary:\n",
    "        model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = []\n",
    "train_scores = []\n",
    "estimators = []\n",
    "\n",
    "ATTRIBUTE_MODEL_PARAMS = [\n",
    "    dict(dense_dropout_rate=0.7),\n",
    "    dict(dense_dropout_rate=0.7),\n",
    "    dict(dense_dropout_rate=0.7),\n",
    "    dict(dense_dropout_rate=0.7),\n",
    "    dict(include_hidden_layer=True, dense_dropout_rate=0.2),\n",
    "]\n",
    "\n",
    "for idx, att in enumerate(ATTRIBUTE_LIST):\n",
    "    print(\"Training for attribute {}\".format(att))\n",
    "    model_params = ATTRIBUTE_MODEL_PARAMS[idx]\n",
    "    \n",
    "    clf = KerasRegressor(\n",
    "        build_fn=lambda: ElmoRegressionModel(**model_params),\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "    clf.fit(X_train, Y_train[:,idx], validation_data=(X_test, Y_test[:,idx]))\n",
    "    estimators.append(clf)\n",
    "\n",
    "    preds_test = clf.predict(X_test)\n",
    "    preds_train = clf.predict(X_train)\n",
    "    df_test[att + \"_Pred\"] = clf.predict(X_dev)\n",
    "    df_dev[att + \"_Pred\"] = clf.predict(X_dev_)\n",
    "    \n",
    "    pearson_r_test = pearsonr(Y_test[:,idx], preds_test)\n",
    "    pearson_r_train = pearsonr(Y_train[:,idx], preds_train)\n",
    "    \n",
    "    test_scores.append(pearson_r_test)\n",
    "    train_scores.append(pearson_r_train)\n",
    "    \n",
    "    print(\"{0} - Test r: {1}\".format(att, pearson_r_test))\n",
    "    print(\"{0} - Train r: {1}\".format(att, pearson_r_train))\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"Average Test r: {}\".format(sum([ts[0] for ts in test_scores]) / len(test_scores)))\n",
    "print(\"Average Train r: {}\".format(sum([ts[0] for ts in train_scores]) / len(train_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\n",
    "    \"preds_test_01.csv\",\n",
    "    columns=[\"Respondent_ID\", *[sym + \"_Pred\" for sym in ATTRIBUTE_LIST]],\n",
    "    index=False\n",
    ")\n",
    "\n",
    "df_dev.to_csv(\n",
    "    \"preds_dev_01.csv\",\n",
    "    columns=[\"Respondent_ID\", *[sym + \"_Pred\" for sym in ATTRIBUTE_LIST]],\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winning submission\n",
    "\n",
    "Each of the three sets of predicted values generated from the above code were submitted to the private leader board. With the exception of Openness, the best predictors from those were then averaged together to form a fourth submission. Our openness predictor was poor, so we continued to tinker with it on the fourth submission. In all cases the averaged values had stronger correlations than the independent values.\n",
    "\n",
    "The final submission was as follows:\n",
    "- Openness: Word List\n",
    "- Concientiousness: averaged the z-transformed predicted values from World List and Deep Learning\n",
    "- Agreeableness: averaged the z-transformed predicted values from Machine Learning and Deep Learning\n",
    "- Extraversion: averaged the z-transformed predicted values from Word List and Deep Learning\n",
    "- Neuroticism: averaged the z-transformed predicted values from Word List and Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
